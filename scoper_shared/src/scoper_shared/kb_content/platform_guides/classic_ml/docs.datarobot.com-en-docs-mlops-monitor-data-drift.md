[ DataRobot docs ](../../index.html "DataRobot docs")

  * [ Data ](../../data/index.html) [ Data ](../../data/index.html)
    * [ Data connections ](../../data/connect-data/index.html) [ Data connections ](../../data/connect-data/index.html)
      * [ Share secure configurations ](../../data/connect-data/secure-config.html)
      * [ Data connections ](../../data/connect-data/data-conn.html)
    * [ AI Catalog ](../../data/ai-catalog/index.html) [ AI Catalog ](../../data/ai-catalog/index.html)
      * [ Load data ](../../data/ai-catalog/catalog.html)
      * [ Manage assets ](../../data/ai-catalog/manage-asset.html)
      * [ Work with assets ](../../data/ai-catalog/catalog-asset.html)
      * [ Schedule snapshots ](../../data/ai-catalog/snapshot.html)
      * [ Prepare data with Spark SQL ](../../data/ai-catalog/spark.html)
    * [ Import data ](../../data/import-data/index.html) [ Import data ](../../data/import-data/index.html)
      * [ Import to DataRobot directly ](../../data/import-data/import-to-dr.html)
      * [ Large datasets ](../../data/import-data/large-data/index.html) [ Large datasets ](../../data/import-data/large-data/index.html)
        * [ Fast EDA for large datasets ](../../data/import-data/large-data/fast-eda.html)
    * [ Transform data ](../../data/transform-data/index.html) [ Transform data ](../../data/transform-data/index.html)
      * [ Interaction-based transformations ](../../data/transform-data/feature-disc.html)
      * [ Feature Discovery ](../../data/transform-data/feature-discovery/index.html) [ Feature Discovery ](../../data/transform-data/feature-discovery/index.html)
        * [ End-to-end Feature Discovery ](../../data/transform-data/feature-discovery/enrich-data-using-feature-discovery.html)
        * [ Set up Feature Discovery projects ](../../data/transform-data/feature-discovery/fd-overview.html)
        * [ Snowflake integration ](../../data/transform-data/feature-discovery/fd-snowflake.html)
        * [ Feature Discovery settings ](../../data/transform-data/feature-discovery/fd-adv-opt.html)
        * [ Time-aware feature engineering ](../../data/transform-data/feature-discovery/fd-time.html)
        * [ Derived features ](../../data/transform-data/feature-discovery/fd-gen.html)
        * [ Predictions ](../../data/transform-data/feature-discovery/fd-predict.html)
      * [ Manual transformations ](../../data/transform-data/feature-transforms.html)
    * [ Analyze data ](../../data/analyze-data/index.html) [ Analyze data ](../../data/analyze-data/index.html)
      * [ Assess data quality with EDA ](../../data/analyze-data/assess-data-quality-eda.html)
      * [ Analyze features using histograms ](../../data/analyze-data/analyze-histogram.html)
      * [ Analyze frequent values ](../../data/analyze-data/analyze-frequent-values.html)
      * [ Feature details ](../../data/analyze-data/histogram.html)
      * [ Exploratory Spatial Data Analysis (ESDA) ](../../data/analyze-data/lai-esda.html)
      * [ Feature Associations ](../../data/analyze-data/feature-assoc.html)
      * [ Use data pipelines for ingest and transformation ](../../data/analyze-data/pipelines.html)
    * [ Data preview features ](../../data/data-preview/index.html) [ Data preview features ](../../data/data-preview/index.html)
      * [ Create feature lists in the Relationship Editor ](../../data/data-preview/safer-rel-editor-feature-lists.html)
    * [ Data FAQ ](../../data/data-faq.html)
  * [ Modeling ](../../modeling/index.html) [ Modeling ](../../modeling/index.html)
    * [ Build models ](../../modeling/build-models/index.html) [ Build models ](../../modeling/build-models/index.html)
      * [ Build models ](../../modeling/build-models/build-basic/index.html) [ Build models ](../../modeling/build-models/build-basic/index.html)
        * [ Basic model workflow ](../../modeling/build-models/build-basic/model-data.html)
        * [ Work with feature lists ](../../modeling/build-models/build-basic/feature-lists.html)
        * [ Unlock Holdout ](../../modeling/build-models/build-basic/unlocking-holdout.html)
        * [ Comprehensive Autopilot ](../../modeling/build-models/build-basic/more-accuracy.html)
        * [ Add/delete models ](../../modeling/build-models/build-basic/creating-addl-models.html)
        * [ Frozen runs ](../../modeling/build-models/build-basic/frozen-run.html)
        * [ Model Repository ](../../modeling/build-models/build-basic/repository.html)
      * [ Advanced options ](../../modeling/build-models/adv-opt/index.html) [ Advanced options ](../../modeling/build-models/adv-opt/index.html)
        * [ Additional ](../../modeling/build-models/adv-opt/additional.html)
        * [ Bias and Fairness ](../../modeling/build-models/adv-opt/fairness-metrics.html)
        * [ Clustering advanced options ](../../modeling/build-models/adv-opt/time-series-cluster-adv-opt.html)
        * [ External Predictions ](../../modeling/build-models/adv-opt/external-preds.html)
        * [ Feature Constraints ](../../modeling/build-models/adv-opt/feature-con.html)
        * [ Image Augmentation ](../../modeling/build-models/adv-opt/ttia.html)
        * [ Partitioning ](../../modeling/build-models/adv-opt/partitioning.html)
        * [ Smart Downsampling ](../../modeling/build-models/adv-opt/smart-ds.html)
        * [ Time series ](../../modeling/build-models/adv-opt/time-series-adv-opt.html)
        * [ GPUs for deep learning ](../../modeling/build-models/adv-opt/gpus.html)
    * [ Model insights ](../../modeling/analyze-models/index.html) [ Model insights ](../../modeling/analyze-models/index.html)
      * [ Evaluate ](../../modeling/analyze-models/evaluate/index.html) [ Evaluate ](../../modeling/analyze-models/evaluate/index.html)
        * [ Accuracy Over Space ](../../modeling/special-workflows/location-ai/lai-insights.html)
        * [ Accuracy Over Time ](../../modeling/analyze-models/evaluate/aot.html)
        * [ Advanced Tuning ](../../modeling/analyze-models/evaluate/adv-tuning.html)
        * [ Anomaly visualizations ](../../modeling/analyze-models/evaluate/anom-viz.html)
        * [ Confusion Matrix (for multiclass models) ](../../modeling/analyze-models/evaluate/multiclass.html)
        * [ Forecasting Accuracy ](../../modeling/analyze-models/evaluate/forecast-acc.html)
        * [ Forecast vs Actual ](../../modeling/analyze-models/evaluate/fore-act.html)
        * [ Lift Chart ](../../modeling/analyze-models/evaluate/lift-chart.html)
        * [ Period Accuracy ](../../modeling/analyze-models/evaluate/period-accuracy.html)
        * [ Residuals ](../../modeling/analyze-models/evaluate/residuals.html)
        * [ ROC Curve tools ](../../modeling/analyze-models/evaluate/roc-curve-tab/index.html) [ ROC Curve tools ](../../modeling/analyze-models/evaluate/roc-curve-tab/index.html)
          * [ Use the ROC Curve tools ](../../modeling/analyze-models/evaluate/roc-curve-tab/roc-curve-tab-use.html)
          * [ Select data and display threshold ](../../modeling/analyze-models/evaluate/roc-curve-tab/threshold.html)
          * [ Confusion matrix ](../../modeling/analyze-models/evaluate/roc-curve-tab/confusion-matrix.html)
          * [ Prediction Distribution graph ](../../modeling/analyze-models/evaluate/roc-curve-tab/pred-dist-graph.html)
          * [ ROC curve ](../../modeling/analyze-models/evaluate/roc-curve-tab/roc-curve.html)
          * [ Profit curve ](../../modeling/analyze-models/evaluate/roc-curve-tab/profit-curve.html)
          * [ Cumulative Charts ](../../modeling/analyze-models/evaluate/roc-curve-tab/cumulative-charts.html)
          * [ Custom charts ](../../modeling/analyze-models/evaluate/roc-curve-tab/custom-charts.html)
          * [ Metrics ](../../modeling/analyze-models/evaluate/roc-curve-tab/metrics.html)
        * [ Series Insights (clustering) ](../../modeling/analyze-models/evaluate/series-insights.html)
        * [ Series Insights (multiseries) ](../../modeling/analyze-models/evaluate/series-insights-multi.html)
        * [ Stability ](../../modeling/analyze-models/evaluate/stability.html)
        * [ Training Dashboard ](../../modeling/analyze-models/evaluate/training-dash.html)
      * [ Understand ](../../modeling/analyze-models/understand/index.html) [ Understand ](../../modeling/analyze-models/understand/index.html)
        * [ Cluster Insights ](../../modeling/analyze-models/understand/cluster-insights.html)
        * [ Feature Effects ](../../modeling/analyze-models/understand/feature-effects.html)
        * [ Feature Impact ](../../modeling/analyze-models/understand/feature-impact.html)
        * [ Prediction Explanations ](../../modeling/analyze-models/understand/pred-explain/index.html) [ Prediction Explanations ](../../modeling/analyze-models/understand/pred-explain/index.html)
          * [ Prediction Explanations overview ](../../modeling/analyze-models/understand/pred-explain/predex-overview.html)
          * [ SHAP Prediction Explanations ](../../modeling/analyze-models/understand/pred-explain/shap-pe.html)
          * [ XEMP Prediction Explanations ](../../modeling/analyze-models/understand/pred-explain/xemp-pe.html)
          * [ Text Prediction Explanations ](../../modeling/analyze-models/understand/pred-explain/predex-text.html)
          * [ Prediction Explanations for clusters ](../../modeling/analyze-models/understand/pred-explain/cluster-pe.html)
          * [ Prediction Explanations for time-aware projects ](../../modeling/analyze-models/understand/pred-explain/ts-otv-predex.html)
        * [ Word Cloud ](../../modeling/analyze-models/understand/word-cloud.html)
      * [ Describe ](../../modeling/analyze-models/describe/index.html) [ Describe ](../../modeling/analyze-models/describe/index.html)
        * [ Blueprint ](../../modeling/analyze-models/describe/blueprints.html)
        * [ Blueprint JSON ](../../modeling/analyze-models/describe/blueprint-json.html)
        * [ Coefficients (preprocessing) ](../../modeling/analyze-models/describe/coefficients.html)
        * [ Constraints (monotonic) ](../../modeling/analyze-models/describe/monotonic.html)
        * [ Data Quality Handling Report ](../../modeling/analyze-models/describe/dq-report.html)
        * [ Eureqa Models ](../../modeling/analyze-models/describe/eureqa.html)
        * [ Log ](../../modeling/analyze-models/describe/log.html)
        * [ Model Info ](../../modeling/analyze-models/describe/model-info.html)
        * [ Rating Tables ](../../modeling/analyze-models/describe/rating-table.html)
        * [ GA2M output (from Rating Tables) ](../../modeling/analyze-models/describe/ga2m.html)
      * [ Predict ](../../modeling/analyze-models/predictions/index.html) [ Predict ](../../modeling/analyze-models/predictions/index.html)
        * [ Deploy ](../../modeling/analyze-models/predictions/deploy.html)
        * [ Downloads ](../../modeling/analyze-models/predictions/download.html)
        * [ Make Predictions ](../../modeling/analyze-models/predictions/predict.html)
        * [ Portable Predictions ](../../modeling/analyze-models/predictions/port-pred.html)
      * [ Compliance ](../../modeling/analyze-models/compliance/index.html) [ Compliance ](../../modeling/analyze-models/compliance/index.html)
        * [ Model Compliance ](../../modeling/analyze-models/compliance/compliance.html)
        * [ Template Builder for compliance reports ](../../modeling/analyze-models/compliance/template-builder.html)
      * [ Comments ](../../modeling/analyze-models/comments/index.html)
      * [ Bias and Fairness ](../../modeling/analyze-models/bias/index.html) [ Bias and Fairness ](../../modeling/analyze-models/bias/index.html)
        * [ Cross-Class Accuracy ](../../modeling/analyze-models/bias/cross-acc.html)
        * [ Cross-Class Data Disparity ](../../modeling/analyze-models/bias/cross-data.html)
        * [ Per-Class Bias ](../../modeling/analyze-models/bias/per-class.html)
      * [ Other ](../../modeling/analyze-models/other/index.html) [ Other ](../../modeling/analyze-models/other/index.html)
        * [ Bias vs Accuracy ](../../modeling/analyze-models/other/bias-tab.html)
        * [ Insights ](../../modeling/analyze-models/other/analyze-insights.html)
        * [ Learning Curves ](../../modeling/analyze-models/other/learn-curve.html)
        * [ Model Comparison ](../../modeling/analyze-models/other/model-compare.html)
        * [ Speed vs Accuracy ](../../modeling/analyze-models/other/speed.html)
    * [ Specialized workflows ](../../modeling/special-workflows/index.html) [ Specialized workflows ](../../modeling/special-workflows/index.html)
      * [ Bias and Fairness resources ](../../modeling/special-workflows/bias-resources.html)
      * [ Composable ML ](../../modeling/special-workflows/cml/index.html) [ Composable ML ](../../modeling/special-workflows/cml/index.html)
        * [ Composable ML overview ](../../modeling/special-workflows/cml/cml-overview.html)
        * [ Composable ML Quickstart ](../../modeling/special-workflows/cml/cml-quickstart.html)
        * [ Modify a blueprint ](../../modeling/special-workflows/cml/cml-blueprint-edit.html)
        * [ Create custom tasks ](../../modeling/special-workflows/cml/cml-custom-tasks.html)
        * [ Custom environments ](../../modeling/special-workflows/cml/cml-custom-env.html)
        * [ DRUM CLI tool ](../../modeling/special-workflows/cml/cml-drum.html)
        * [ Enable network access for custom tasks ](../../modeling/special-workflows/cml/custom-task-network-access.html)
      * [ Document AI ](../../modeling/special-workflows/doc-ai/index.html) [ Document AI ](../../modeling/special-workflows/doc-ai/index.html)
        * [ Document AI overview ](../../modeling/special-workflows/doc-ai/doc-ai-overview.html)
        * [ Document ingest and modeling ](../../modeling/special-workflows/doc-ai/doc-ai-ingest.html)
        * [ Document AI insights ](../../modeling/special-workflows/doc-ai/doc-ai-insights.html)
        * [ Predictions from documents ](../../modeling/special-workflows/doc-ai/doc-ai-predictions.html)
      * [ Location AI ](../../modeling/special-workflows/location-ai/index.html) [ Location AI ](../../modeling/special-workflows/location-ai/index.html)
        * [ Data ingest ](../../modeling/special-workflows/location-ai/lai-ingest.html)
        * [ Exploratory Spatial Data Analysis (ESDA) ](../../modeling/special-workflows/location-ai/lai-esda.html)
        * [ Modeling ](../../modeling/special-workflows/location-ai/lai-model.html)
        * [ Accuracy Over Space ](../../modeling/special-workflows/location-ai/lai-insights.html)
      * [ Unsupervised learning ](../../modeling/special-workflows/unsupervised/index.html) [ Unsupervised learning ](../../modeling/special-workflows/unsupervised/index.html)
        * [ Anomaly detection ](../../modeling/special-workflows/unsupervised/anomaly-detection.html)
        * [ Clustering ](../../modeling/special-workflows/unsupervised/clustering.html)
      * [ Visual AI ](../../modeling/special-workflows/visual-ai/index.html) [ Visual AI ](../../modeling/special-workflows/visual-ai/index.html)
        * [ Visual AI overview ](../../modeling/special-workflows/visual-ai/vai-overview.html)
        * [ Build Visual AI models ](../../modeling/special-workflows/visual-ai/vai-model.html)
        * [ Train-time image augmentation ](../../modeling/special-workflows/visual-ai/tti-augment/index.html) [ Train-time image augmentation ](../../modeling/special-workflows/visual-ai/tti-augment/index.html)
          * [ About augmented models ](../../modeling/special-workflows/visual-ai/tti-augment/ttia-introduction.html)
          * [ Transformations and lists ](../../modeling/special-workflows/visual-ai/tti-augment/ttia-lists.html)
          * [ Use case examples ](../../modeling/special-workflows/visual-ai/tti-augment/ttia-examples.html)
        * [ Model insights ](../../modeling/special-workflows/visual-ai/vai-insights.html)
        * [ Tune models ](../../modeling/special-workflows/visual-ai/vai-tuning.html)
        * [ Visual AI predictions ](../../modeling/special-workflows/visual-ai/vai-predictions.html)
      * [ Multilabel modeling ](../../modeling/special-workflows/multilabel.html)
      * [ Out-of-time validation modeling ](../../modeling/special-workflows/otv.html)
      * [ Text AI resources ](../../modeling/special-workflows/textai-resources.html)
    * [ Time-series modeling ](../../modeling/time/index.html) [ Time-series modeling ](../../modeling/time/index.html)
      * [ What is time-aware modeling? ](../../modeling/time/whatis-time.html)
      * [ Time series modeling ](../../modeling/time/ts-flow-overview.html)
      * [ Time series insights ](../../modeling/time/ts-leaderboard.html)
      * [ Time series predictions ](../../modeling/time/ts-predictions.html)
      * [ Time series portable predictions with prediction intervals ](../../modeling/time/ts-port-pred-intervals.html)
      * [ Multiseries modeling ](../../modeling/time/multiseries.html)
      * [ Clustering ](../../modeling/time/ts-clustering.html)
      * [ Segmented modeling ](../../modeling/time/ts-segmented.html)
      * [ Nowcasting ](../../modeling/time/nowcasting.html)
      * [ External prediction comparison ](../../modeling/time/cyob.html)
      * [ Batch predictions for TTS and LSTM models ](../../modeling/time/ts-tts-lstm-batch-pred.html)
      * [ Time series advanced modeling ](../../modeling/time/ts-adv-modeling/index.html) [ Time series advanced modeling ](../../modeling/time/ts-adv-modeling/index.html)
        * [ Time series advanced options ](../../modeling/time/ts-adv-modeling/ts-adv-opt.html)
        * [ Clustering advanced options ](../../modeling/time/ts-adv-modeling/ts-cluster-adv-opt.html)
        * [ Date/time partitioning advanced options ](../../modeling/time/ts-adv-modeling/ts-date-time.html)
        * [ Customizing time series projects ](../../modeling/time/ts-adv-modeling/ts-customization.html)
      * [ Time series modeling data ](../../modeling/time/ts-modeling-data/index.html) [ Time series modeling data ](../../modeling/time/ts-modeling-data/index.html)
        * [ Create the modeling dataset ](../../modeling/time/ts-modeling-data/ts-create-data.html)
        * [ Data prep for time series ](../../modeling/time/ts-modeling-data/ts-data-prep.html)
        * [ Restore features removed by reduction ](../../modeling/time/ts-modeling-data/restore-features.html)
    * [ AutoML preview features ](../../modeling/automl-preview/index.html) [ AutoML preview features ](../../modeling/automl-preview/index.html)
      * [ Quantile regression analysis ](../../modeling/automl-preview/quantile-reg.html)
      * [ Configure hyperparameters for custom tasks ](../../modeling/automl-preview/cml-hyperparam.html)
    * [ Modeling FAQ ](../../modeling/general-modeling-faq.html)
    * [ Value Tracker ](../../modeling/value-tracker.html)
    * [ Project control center ](../../modeling/manage-projects.html)
  * [ Predictions ](../../predictions/index.html) [ Predictions ](../../predictions/index.html)
    * [ Real-time scoring methods ](../../predictions/realtime/index.html) [ Real-time scoring methods ](../../predictions/realtime/index.html)
      * [ Prediction API snippets ](../../predictions/realtime/code-py.html)
      * [ Qlik predictions ](../../predictions/realtime/integration-code-snippets.html)
    * [ Batch prediction methods ](../../predictions/batch/index.html) [ Batch prediction methods ](../../predictions/batch/index.html)
      * [ Batch prediction UI ](../../predictions/batch/batch-dep/index.html) [ Batch prediction UI ](../../predictions/batch/batch-dep/index.html)
        * [ Make a one-time batch prediction ](../../predictions/batch/batch-dep/batch-pred.html)
        * [ Schedule recurring batch prediction jobs ](../../predictions/batch/batch-dep/batch-pred-jobs.html)
        * [ Manage prediction job definitions ](../../predictions/batch/batch-dep/manage-pred-job-def.html)
        * [ Snowflake prediction job examples ](../../predictions/batch/batch-dep/pred-job-examples-snowflake.html)
      * [ Prediction monitoring jobs ](../../predictions/batch/pred-monitoring-jobs/index.html) [ Prediction monitoring jobs ](../../predictions/batch/pred-monitoring-jobs/index.html)
        * [ Create monitoring jobs ](../../predictions/batch/pred-monitoring-jobs/ui-monitoring-jobs.html)
        * [ Monitoring jobs API ](../../predictions/batch/pred-monitoring-jobs/api-monitoring-jobs.html)
        * [ Manage monitoring job definitions ](../../predictions/batch/pred-monitoring-jobs/manage-monitoring-job-def.html)
      * [ Manage batch jobs ](../../predictions/batch/batch-jobs.html)
      * [ Batch prediction scripts ](../../predictions/batch/cli-scripts.html)
    * [ Portable prediction methods ](../../predictions/port-pred/index.html) [ Portable prediction methods ](../../predictions/port-pred/index.html)
      * [ Scoring Code ](../../predictions/port-pred/scoring-code/index.html) [ Scoring Code ](../../predictions/port-pred/scoring-code/index.html)
        * [ Download Scoring Code from the Leaderboard ](../../predictions/port-pred/scoring-code/sc-download-leaderboard.html)
        * [ Download Scoring Code from a deployment ](../../predictions/port-pred/scoring-code/sc-download-deployment.html)
        * [ Download Scoring Code from the Leaderboard (Legacy) ](../../predictions/port-pred/scoring-code/sc-download-legacy.html)
        * [ Scoring Code for time series projects ](../../predictions/port-pred/scoring-code/sc-time-series.html)
        * [ Scoring at the command line ](../../predictions/port-pred/scoring-code/scoring-cli.html)
        * [ Scoring Code usage examples ](../../predictions/port-pred/scoring-code/quickstart-api.html)
        * [ JAR structure ](../../predictions/port-pred/scoring-code/jar-package.html)
        * [ Generate Java models in an existing project ](../../predictions/port-pred/scoring-code/build-verify.html)
        * [ Backward-compatible Java API ](../../predictions/port-pred/scoring-code/java-back-compat.html)
        * [ Scoring Code JAR integrations ](../../predictions/port-pred/scoring-code/sc-jar-integrations.html)
        * [ Android integration ](../../predictions/port-pred/scoring-code/android.html)
      * [ Portable Prediction Server ](../../predictions/port-pred/pps/index.html) [ Portable Prediction Server ](../../predictions/port-pred/pps/index.html)
        * [ Portable Prediction Server configuration ](../../predictions/port-pred/pps/portable-pps.html)
        * [ Portable Prediction Server running modes ](../../predictions/port-pred/pps/pps-run-modes.html)
        * [ Portable batch predictions ](../../predictions/port-pred/pps/portable-batch-predictions.html)
        * [ Custom model Portable Prediction Server ](../../predictions/port-pred/pps/custom-pps.html)
      * [ DataRobot Prime (deprecated) ](../../predictions/port-pred/prime/index.html)
    * [ Predictions testing ](../../predictions/pred-test.html)
    * [ Predictions reference ](../../predictions/pred-file-limits.html)
  * [ MLOps ](../index.html) [ MLOps ](../index.html)
    * [ Deployment ](../deployment/index.html) [ Deployment ](../deployment/index.html)
      * [ Deployment workflows ](../deployment/deploy-workflows/index.html) [ Deployment workflows ](../deployment/deploy-workflows/index.html)
        * [ DataRobot model in a DataRobot environment ](../deployment/deploy-workflows/dr-model-dr-env.html)
        * [ DataRobot model in a PPS ](../deployment/deploy-workflows/dr-model-pps-env.html)
        * [ Custom model in a DataRobot environment ](../deployment/deploy-workflows/cus-model-dr-env.html)
        * [ Custom model in a PPS ](../deployment/deploy-workflows/cus-model-pps-env.html)
        * [ Scoring Code in an external environment ](../deployment/deploy-workflows/ext-dr-model-ext-env.html)
        * [ Monitor an external model with the monitoring agent ](../deployment/deploy-workflows/ext-cus-model-ext-env.html)
      * [ Register models ](../deployment/registry/index.html) [ Register models ](../deployment/registry/index.html)
        * [ Model Registry ](../deployment/registry/reg-create.html)
        * [ Register DataRobot models ](../deployment/registry/dr-model-reg.html)
        * [ Register custom models ](../deployment/registry/reg-custom-models.html)
        * [ Register external models ](../deployment/registry/reg-external-models.html)
        * [ Deploy registered models ](../deployment/registry/reg-deploy.html)
        * [ View and manage registered models ](../deployment/registry/reg-action.html)
        * [ Generate model compliance documentation ](../deployment/registry/reg-compliance.html)
        * [ Extend compliance documentation with key values ](../deployment/registry/reg-key-values.html)
        * [ Custom jobs ](../deployment/registry/reg-custom-jobs.html)
        * [ Import model packages into MLOps ](../deployment/registry/reg-transfer.html)
        * [ Model logs for model packages (legacy) ](../deployment/registry/reg-model-pkg-logs.html)
      * [ Prepare custom models for deployment ](../deployment/custom-models/index.html) [ Prepare custom models for deployment ](../deployment/custom-models/index.html)
        * [ Custom Model Workshop ](../deployment/custom-models/custom-model-workshop/index.html) [ Custom Model Workshop ](../deployment/custom-models/custom-model-workshop/index.html)
          * [ Create custom inference models ](../deployment/custom-models/custom-model-workshop/custom-inf-model.html)
          * [ Manage custom model dependencies ](../deployment/custom-models/custom-model-workshop/custom-model-dependencies.html)
          * [ Manage custom model resources ](../deployment/custom-models/custom-model-workshop/custom-model-resource-mgmt.html)
          * [ Add custom model versions ](../deployment/custom-models/custom-model-workshop/custom-model-versions.html)
          * [ Add training data to a custom model ](../deployment/custom-models/custom-model-workshop/custom-model-training-data.html)
          * [ Add files from remote repos to custom models ](../deployment/custom-models/custom-model-workshop/custom-model-repos.html)
          * [ Test custom models ](../deployment/custom-models/custom-model-workshop/custom-model-test.html)
          * [ Manage custom models ](../deployment/custom-models/custom-model-workshop/custom-model-actions.html)
          * [ Register custom models ](../deployment/custom-models/custom-model-workshop/custom-model-reg.html)
          * [ Create custom model proxies for external models ](../deployment/custom-models/custom-model-workshop/ext-model-proxy.html)
          * [ GitHub Actions for custom models ](../deployment/custom-models/custom-model-workshop/custom-model-github-action.html)
        * [ Custom model environments ](../deployment/custom-models/custom-model-environments/index.html) [ Custom model environments ](../deployment/custom-models/custom-model-environments/index.html)
          * [ Drop-in environments ](../deployment/custom-models/custom-model-environments/drop-in-environments.html)
          * [ Custom environments ](../deployment/custom-models/custom-model-environments/custom-environments.html)
      * [ Prepare for external model deployment ](../deployment/ext-model-prep/index.html) [ Prepare for external model deployment ](../deployment/ext-model-prep/index.html)
        * [ Add external prediction environments ](../deployment/ext-model-prep/ext-pred-env.html)
        * [ Manage prediction environments ](../deployment/ext-model-prep/ext-pred-env-manage.html)
        * [ Register external models ](../deployment/ext-model-prep/ext-model-reg.html)
      * [ Manage prediction environments ](../deployment/prediction-env/index.html) [ Manage prediction environments ](../deployment/prediction-env/index.html)
        * [ Add DataRobot Serverless prediction environments ](../deployment/prediction-env/pred-env.html)
        * [ Add external prediction environments ](../deployment/prediction-env/ext-pred-env.html)
        * [ Manage prediction environments ](../deployment/prediction-env/pred-env-manage.html)
        * [ Deploy a model to a prediction environment ](../deployment/prediction-env/pred-env-deploy.html)
        * [ Prediction environment integrations ](../deployment/prediction-env/pred-env-integrations/index.html) [ Prediction environment integrations ](../deployment/prediction-env/pred-env-integrations/index.html)
          * [ Automated deployment and replacement of Scoring Code in AzureML ](../deployment/prediction-env/pred-env-integrations/azureml-sc-deploy-replace.html)
          * [ Automated deployment and replacement in Sagemaker ](../deployment/prediction-env/pred-env-integrations/sagemaker-cm-deploy-replace.html)
      * [ Deploy models ](../deployment/deploy-methods/index.html) [ Deploy models ](../deployment/deploy-methods/index.html)
        * [ Deploy DataRobot models ](../deployment/deploy-methods/deploy-model.html)
        * [ Deploy custom models ](../deployment/deploy-methods/deploy-custom-inf-model.html)
        * [ Deploy external models ](../deployment/deploy-methods/deploy-external-model.html)
        * [ Configure deployment settings ](../deployment/deploy-methods/add-deploy-info.html)
        * [ Add prediction data post-deployment ](../deployment/deploy-methods/add-prediction-data-post-deploy.html)
      * [ MLOps agents ](../deployment/mlops-agent/index.html) [ MLOps agents ](../deployment/mlops-agent/index.html)
        * [ Monitoring agent ](../deployment/mlops-agent/monitoring-agent/index.html) [ Monitoring agent ](../deployment/mlops-agent/monitoring-agent/index.html)
          * [ Installation and configuration ](../deployment/mlops-agent/monitoring-agent/agent.html)
          * [ Examples directory ](../deployment/mlops-agent/monitoring-agent/agent-ex.html)
          * [ Monitoring agent use cases ](../deployment/mlops-agent/monitoring-agent/agent-use.html)
          * [ Environment variables ](../deployment/mlops-agent/monitoring-agent/env-var.html)
          * [ Library and agent spooler configuration ](../deployment/mlops-agent/monitoring-agent/spooler.html)
          * [ Download Scoring Code ](../deployment/mlops-agent/monitoring-agent/agent-sc.html)
          * [ Monitoring external multiclass deployments ](../deployment/mlops-agent/monitoring-agent/agent-multi.html)
        * [ Management agent ](../deployment/mlops-agent/mgmt-agent/index.html) [ Management agent ](../deployment/mlops-agent/mgmt-agent/index.html)
          * [ Installation and configuration ](../deployment/mlops-agent/mgmt-agent/mgmt-agent-install.html)
          * [ Configure environment plugins ](../deployment/mlops-agent/mgmt-agent/mgmt-agent-plugins.html)
          * [ Install the management agent for Kubernetes ](../deployment/mlops-agent/mgmt-agent/mgmt-agent-kubernetes.html)
          * [ Management agent deployment status and events ](../deployment/mlops-agent/mgmt-agent/mgmt-agent-events-status.html)
          * [ Relaunch deployments ](../deployment/mlops-agent/mgmt-agent/mgmt-agent-relaunch.html)
          * [ Force delete deployments ](../deployment/mlops-agent/mgmt-agent/mgmt-agent-delete.html)
        * [ Agent event log ](../deployment/mlops-agent/agent-event-log.html)
    * [ Deployment settings ](../deployment-settings/index.html) [ Deployment settings ](../deployment-settings/index.html)
      * [ Set up service health monitoring ](../deployment-settings/service-health-settings.html)
      * [ Set up data drift monitoring ](../deployment-settings/data-drift-settings.html)
      * [ Set up accuracy monitoring ](../deployment-settings/accuracy-settings.html)
      * [ Set up fairness monitoring ](../deployment-settings/fairness-settings.html)
      * [ Set up humility rules ](../deployment-settings/humility-settings.html)
      * [ Configure retraining ](../deployment-settings/retraining-settings.html)
      * [ Configure challengers ](../deployment-settings/challengers-settings.html)
      * [ Configure predictions settings ](../deployment-settings/predictions-settings.html)
      * [ Enable data exploration ](../deployment-settings/data-exploration-settings.html)
      * [ Set up custom metrics monitoring ](../deployment-settings/custom-metrics-settings.html)
      * [ Set up timeliness tracking ](../deployment-settings/usage-settings.html)
    * [ Lifecycle management ](../manage-mlops/index.html) [ Lifecycle management ](../manage-mlops/index.html)
      * [ Deployment inventory ](../manage-mlops/deploy-inventory.html)
      * [ Manage deployments ](../manage-mlops/actions-menu.html)
      * [ Replace deployed models ](../manage-mlops/deploy-replace.html)
      * [ Manage Automated Retraining policies ](../manage-mlops/set-up-auto-retraining.html)
    * [ Performance monitoring ](index.html) [ Performance monitoring ](index.html)
      * [ Overview tab ](dep-overview.html)
      * [ Accuracy tab ](deploy-accuracy.html)
      * [ Data Drift tab ](data-drift.html) [ Data Drift tab ](data-drift.html)

[For Self-Managed AI Platform users running v11.1, see the on-premise platform
documentation __](/11.1/en/docs/index.html)

Data Drift tab

        * Configure the Data Drift dashboard 
        * Feature Drift vs Feature Importance chart 
          * Feature Drift 
          * Feature Importance 
          * Interpret the quadrants 
        * Feature Details chart 
          * Numeric features 
          * Categorical features 
          * Text features 
        * Drift Over Time chart 
        * Predictions Over Time chart 
          * For regression projects 
          * For binary classification projects 
          * Prediction warnings integration 
        * Use the version selector 
        * Use the time range and resolution dropdowns 
        * Use the date slider 
        * Use the class selector 
        * Drill down on the Data Drift tab 
          * Configure the drill down display settings 
          * Use the feature drift heat map 
          * Use the feature drift comparison chart 

      * [ Service Health tab ](service-health.html)
      * [ Challengers tab ](challengers.html)
      * [ Usage tab ](deploy-usage.html)
      * [ Data Exploration tab ](data-exploration.html)
      * [ Custom Metrics tab ](custom-metrics.html)
      * [ Segmented analysis ](deploy-segment.html)
      * [ Batch monitoring ](deploy-batch-monitor.html)
      * [ Generative model monitoring ](generative-model-monitoring.html)
    * [ Governance ](../governance/index.html) [ Governance ](../governance/index.html)
      * [ Model deployment approval workflow ](../governance/dep-admin.html)
      * [ Governance lens ](../governance/gov-lens.html)
      * [ Notifications tab ](../governance/deploy-notifications.html)
      * [ Humility tab ](../governance/humble.html)
      * [ Fairness tab ](../governance/mlops-fairness.html)
      * [ Deployment reports ](../governance/deploy-reports.html)
    * [ MLOps preview features ](../mlops-preview/index.html) [ MLOps preview features ](../mlops-preview/index.html)
      * [ Service Health and Accuracy history ](../mlops-preview/pp-deploy-history.html)
      * [ Automated deployment and replacement of Scoring Code in Snowflake ](../mlops-preview/pp-snowflake-sc-deploy-replace.html)
      * [ Run the monitoring agent in DataRobot ](../mlops-preview/monitoring-agent-in-dr.html)
      * [ Feature cache for Feature Discovery deployments ](../mlops-preview/safer-ft-cache.html)
      * [ MLOps reporting for unstructured models ](../mlops-preview/mlops-unstructured-models.html)
    * [ MLOps FAQ ](../mlops-faq.html)
  * [ Notebooks ](../../dr-notebooks/index.html) [ Notebooks ](../../dr-notebooks/index.html)
    * [ Manage notebooks ](../../dr-notebooks/manage-nb/index.html) [ Manage notebooks ](../../dr-notebooks/manage-nb/index.html)
      * [ Add notebooks ](../../dr-notebooks/manage-nb/dr-create-nb.html)
      * [ Notebook settings ](../../dr-notebooks/manage-nb/dr-settings-nb.html)
      * [ Notebook versioning ](../../dr-notebooks/manage-nb/dr-revise-nb.html)
    * [ Notebook coding experience ](../../dr-notebooks/code-nb/index.html) [ Notebook coding experience ](../../dr-notebooks/code-nb/index.html)
      * [ Environment management ](../../dr-notebooks/code-nb/dr-env-nb.html)
      * [ Create and execute cells ](../../dr-notebooks/code-nb/dr-cell-nb.html)
      * [ Cell actions ](../../dr-notebooks/code-nb/dr-action-nb.html)
      * [ Code intelligence ](../../dr-notebooks/code-nb/dr-code-int.html)
      * [ Notebook terminals ](../../dr-notebooks/code-nb/dr-terminal-nb.html)
      * [ Azure OpenAI Service integration ](../../dr-notebooks/code-nb/dr-openai-nb.html)
    * [ Notebook reference ](../../dr-notebooks/dr-notebook-ref.html)
  * [ AI Apps ](../../app-builder/index.html) [ AI Apps ](../../app-builder/index.html)
    * [ Create applications ](../../app-builder/create-app.html)
    * [ Manage applications ](../../app-builder/current-app.html)
    * [ Edit no-code applications ](../../app-builder/edit-apps/index.html) [ Edit no-code applications ](../../app-builder/edit-apps/index.html)
      * [ Pages ](../../app-builder/edit-apps/app-pages.html)
      * [ Widgets ](../../app-builder/edit-apps/app-widgets.html)
      * [ What-if and Optimizer ](../../app-builder/edit-apps/whatif-opt.html)
      * [ Settings ](../../app-builder/edit-apps/app-settings.html)
    * [ Use no-code applications ](../../app-builder/use-apps/index.html) [ Use no-code applications ](../../app-builder/use-apps/index.html)
      * [ Make predictions ](../../app-builder/use-apps/app-make-pred.html)
      * [ View prediction results ](../../app-builder/use-apps/app-analyze-result.html)
    * [ Time series applications ](../../app-builder/ts-app.html)
    * [ Custom apps ](../../app-builder/custom-apps/index.html) [ Custom apps ](../../app-builder/custom-apps/index.html)
      * [ Upload custom applications ](../../app-builder/custom-apps/app-upload-custom.html)
      * [ Host custom applications ](../../app-builder/custom-apps/custom-apps-hosting.html)
      * [ Manage custom applications ](../../app-builder/custom-apps/manage-custom-apps.html)
    * [ AI App reference ](../../app-builder/reference/index.html) [ AI App reference ](../../app-builder/reference/index.html)
      * [ Default widgets ](../../app-builder/reference/default-widgets.html)
      * [ Optional widgets ](../../app-builder/reference/optional-widgets.html)
    * [ AI App preview features ](../../app-builder/app-preview/index.html) [ AI App preview features ](../../app-builder/app-preview/index.html)
      * [ Prefill application templates ](../../app-builder/app-preview/app-prefill.html)
      * [ Feature Discovery support in No-Code AI Apps ](../../app-builder/app-preview/app-ft-cache.html)
  * [ Integrations ](../../integrations/index.html) [ Integrations ](../../integrations/index.html)
    * [ AWS ](../../integrations/aws/index.html) [ AWS ](../../integrations/aws/index.html)
      * [ Import data from AWS S3 ](../../integrations/aws/import-from-aws-s3.html)
      * [ Deploy models on AWS EKS ](../../integrations/aws/deploy-dr-models-on-aws.html)
      * [ Path-based routing to PPS ](../../integrations/aws/path-based-routing-to-pps-on-aws.html)
      * [ Score Snowflake data on AWS EMR Spark ](../../integrations/aws/score-snowflake-aws-emr-spark.html)
      * [ Ingest data with AWS Athena ](../../integrations/aws/ingest-athena.html)
      * [ AWS Lambda ](../../integrations/aws/lambda/index.html) [ AWS Lambda ](../../integrations/aws/lambda/index.html)
        * [ AWS Lambda reporting to MLOps ](../../integrations/aws/lambda/aws-lambda-reporting-to-mlops.html)
        * [ Use DataRobot Prime models with AWS Lambda ](../../integrations/aws/lambda/prime-lambda.html)
        * [ Use Scoring Code with AWS Lambda ](../../integrations/aws/lambda/sc-lambda.html)
      * [ Amazon SageMaker ](../../integrations/aws/sagemaker/index.html) [ Amazon SageMaker ](../../integrations/aws/sagemaker/index.html)
        * [ Deploy models on SageMaker ](../../integrations/aws/sagemaker/sagemaker-deploy.html)
        * [ Use Scoring Code with AWS SageMaker ](../../integrations/aws/sagemaker/sc-sagemaker.html)
    * [ Azure ](../../integrations/azure/index.html) [ Azure ](../../integrations/azure/index.html)
      * [ Run Batch Prediction jobs from Azure Blob Storage ](../../integrations/azure/azure-blob-storage-batch-pred.html)
      * [ Deploy and monitor DataRobot models in Azure Kubernetes Service ](../../integrations/azure/aks-deploy-and-monitor.html)
      * [ Deploy and monitor Spark models with DataRobot MLOps ](../../integrations/azure/spark-deploy-and-monitor.html)
      * [ Deploy and monitor ML.NET models with DataRobot MLOps ](../../integrations/azure/mlnet-deploy-and-monitor.html)
      * [ Use Scoring Code with Azure ML ](../../integrations/azure/sc-azureml.html)
    * [ Google ](../../integrations/google/index.html) [ Google ](../../integrations/google/index.html)
      * [ Deploy and monitor models on GCP ](../../integrations/google/google-cloud-platform.html)
      * [ Deploy the MLOps agent on GKE ](../../integrations/google/mlops-agent-with-gke.html)
    * [ Snowflake ](../../integrations/snowflake/index.html) [ Snowflake ](../../integrations/snowflake/index.html)
      * [ Data ingest and project creation ](../../integrations/snowflake/sf-project-creation.html)
      * [ Real-time predictions ](../../integrations/snowflake/sf-client-scoring.html)
      * [ Server-side model scoring ](../../integrations/snowflake/sf-server-scoring.html)
      * [ Snowflake external functions and streams ](../../integrations/snowflake/sf-function-streams.html)
      * [ Generate Snowflake UDF Scoring Code ](../../integrations/snowflake/snowflake-sc.html)

[For Self-Managed AI Platform users running v11.1, see the on-premise platform
documentation __](/11.1/en/docs/index.html)

Data Drift tab

  * Configure the Data Drift dashboard 
  * Feature Drift vs Feature Importance chart 
    * Feature Drift 
    * Feature Importance 
    * Interpret the quadrants 
  * Feature Details chart 
    * Numeric features 
    * Categorical features 
    * Text features 
  * Drift Over Time chart 
  * Predictions Over Time chart 
    * For regression projects 
    * For binary classification projects 
    * Prediction warnings integration 
  * Use the version selector 
  * Use the time range and resolution dropdowns 
  * Use the date slider 
  * Use the class selector 
  * Drill down on the Data Drift tab 
    * Configure the drill down display settings 
    * Use the feature drift heat map 
    * Use the feature drift comparison chart 

[MLOps](../index.html "MLOps") > [Performance monitoring](index.html
"Performance monitoring") > Data Drift tab

# Data Drift tab¶

As training and production data change over time, a deployed model loses
predictive power. The data surrounding the model is said to be _drifting_. By
leveraging the training data and prediction data (also known as inference
data) that is added to your deployment, the **Data Drift** dashboard helps you
analyze a model's performance after it has been deployed.

How does DataRobot track drift?

DataRobot tracks two types of drift:

  * **Target drift** : DataRobot stores statistics about predictions to monitor how the distribution and values of the target change over time. As a baseline for comparing target distributions, DataRobot uses the distribution of predictions on the holdout.

  * **Feature drift** : DataRobot stores statistics about predictions to monitor how distributions and values of features change over time. The supported feature data types are numeric, categorical, and text. As a baseline for comparing distributions of features:

    * For training datasets larger than 500MB, DataRobot uses the distribution of a random sample of the training data.

    * For training datasets smaller than 500MB, DataRobot uses the distribution of 100% of the training data.

How many features can DataRobot track?

The following limits apply to tracking and receiving features in DataRobot:

  * _Managed AI Platform (SaaS)_ : By default, DataRobot tracks up to 25 features.

  * _Self-Managed AI Platform (on-premise)_ : By default, DataRobot tracks up to 25 features; however, self-managed installations can increase the limit to 200 features using the `PREDICTION_API_MONITOR_RAW_MAX_FEATURE` setting in the DataRobot configuration. In addition, the maximum number of features that DataRobot can receive is set using `PREDICTION_API_POST_MAX_FEATURES` and the absolute maximum number of features DataRobot can receive is 300. _For[agent-monitored](../deployment/mlops-agent/monitoring-agent/index.html) deployments_, the 300 feature limit applies, even if you configure the agent to send more than 300 features using `MLOPS_MAX_FEATURES_TO_MONITOR`.

Target and feature tracking are enabled by default. You can control these
drift tracking features by navigating to a deployment's [**Data Drift >
Settings**](../deployment-settings/data-drift-settings.html) tab.

Availability information

If feature drift tracking is turned off, a message displays on the **Data
Drift** tab to remind you to enable [feature drift tracking](../deployment-
settings/data-drift-settings.html).

To receive email notifications on data drift status, [configure
notifications](../governance/deploy-notifications.html), [schedule
monitoring](../deployment-settings/data-drift-settings.html#schedule-data-
drift-monitoring-notifications), and [configure data drift monitoring
settings](../deployment-settings/data-drift-settings.html#define-data-drift-
monitoring-notifications).

The **Data Drift** dashboard provides four interactive and exportable
visualizations that help identify the health of a deployed model over a
specified time interval.

Note

The **Export** button allows you to download each chart on the **Data Drift**
dashboard as a PNG, CSV, or ZIP file.

[![](../../images/data-drift-1.png)](../../images/data-drift-1.png)

| Chart | Description  
---|---|---  
1 | Feature Drift vs. Feature Importance | Plots the importance of a feature in a model against how much the distribution of feature values has changed, or drifted, between one point in time and another.  
2 | Feature Details | Plots percentage of records, i.e., the distribution, of the selected feature in the training data compared to the inference data.  
3 | Drift Over Time | Illustrates the difference in distribution over time between the training dataset of the deployed model and the datasets used to generate predictions in production. This chart tracks the change in the Population Stability Index (PSI), which is a measure of [data drift](../../reference/glossary/index.html#data-drift).  
4 | Predictions Over Time | Illustrates how the distribution of a model's predictions has changed over time (_target drift_). The display differs depending on whether the project is regression or binary classification.  
  
In addition to the visualizations above, you can use the **Data Drift > Drill
Down** tab to compare data drift heat maps across the features in a deployment
to identify drift trends.

[![](../../images/pp-drill-down-location.png)](../../images/pp-drill-down-
location.png)

## Configure the Data Drift dashboard¶

You can [customize how a deployment calculates data drift
status](../deployment-settings/data-drift-settings.html) by configuring drift
and importance thresholds and additional definitions on the **Data Drift >
Settings** page. You can also use the following controls to configure the
**Data Drift** dashboard as needed:

[![](../../images/data-drift-19.png)](../../images/data-drift-19.png)

| Control | Description  
---|---|---  
1 | Model version selector | Updates the dashboard displays to reflect the model you selected from the dropdown.  
2 | Date Slider | Limits the range of data displayed on the dashboard (i.e., zooms in on a specific time period).  
3 | Range (UTC) | Sets the date range displayed for the deployment date slider.  
4 | Resolution | Sets the time granularity of the deployment date slider.  
5 | Selected Feature | Sets the feature displayed on the Feature Details chart and the Drift Over Time chart.  
6 | Refresh | Initiates an on-demand update of the dashboard with new data. Otherwise, DataRobot refreshes the dashboard every 15 minutes.  
7 | Reset | Reverts the dashboard controls to the default settings.  
  
The **Data Drift** dashboard also supports [segmented analysis](deploy-
segment.html), allowing you to view data drift while comparing a subset of
training data to the predictions data for individual attributes and values
using the **Segment Attribute** and **Segment Value** dropdowns.

## Feature Drift vs Feature Importance chart¶

The **Feature Drift vs. Feature Importance** chart monitors the 25 most
impactful numerical, categorical, and text-based features in your data.

[![](../../images/data-drift-5.png)](../../images/data-drift-5.png)

Use the chart to see if data is different at one point in time compared to
another. Differences may indicate problems with your model or in the data
itself. For example, if users of an auto insurance product are getting younger
over time, the data that built the original model may no longer result in
accurate predictions for your newer data. Particularly, drift in features with
high importance can be a warning flag about your model accuracy. Hover over a
point in the chart to identify the feature name and report the precise values
for drift (Y-axis) and importance (X-axis).

### Feature Drift¶

The Y-axis reports the **Drift** value for a feature. This value is a
calculation of the [Population Stability Index
(PSI)](https://www.kaggle.com/code/podsyp/population-stability-
index/notebook), a measure of the difference in distribution over time.

Drift metric support

While the DataRobot UI only supports the Population Stability Index (PSI)
metric, the [DataRobot
API](https://docs.datarobot.com/en/docs/api/reference/public-
api/deployments.html#enumerated-values_4) supports Kullback-Leibler
Divergence, Hellinger Distance, Histogram Intersection, and Jensen–Shannon
Divergence. In addition, using the Python API client, you can [retrieve a list
of supported metrics](https://datarobot-public-api-client.readthedocs-
hosted.com/reference/mlops/deployment.html#data-drift).

### Feature Importance¶

The X-axis reports the **Importance** score for a feature, calculated when
ingesting the learning (or training) data. DataRobot calculates feature
importance differently depending on the model type. For DataRobot models and
custom models, the **Importance** score is calculated using [**Permutation
Importance**](../../modeling/analyze-models/understand/feature-
impact.html#permutation-based-feature-impact). For external models, the
importance score is an [**ACE
Score**](../../reference/glossary/index.html#ace-scores). The dot resting at
the Importance value of `1` is the target prediction [![](../../images/icon-
drift-pred.png)](../../images/icon-drift-pred.png). The most important feature
in the model will also appear at 1 (as a solid green dot).

[![](../../images/data-drift-7.png)](../../images/data-drift-7.png)

### Interpret the quadrants¶

The quadrants represented in the chart help to visualize feature-by-feature
data drift plotted against the feature's importance. Quadrants can be loosely
interpreted as follows:

[![](../../images/data-drift-6.png)](../../images/data-drift-6.png)

Quadrant | Read as... | Color indicator  
---|---|---  
1 | High importance feature(s) are experiencing high drift. Investigate immediately. | Red  
2 | Lower importance feature(s) are experiencing drift above the set threshold. Monitor closely. | Yellow  
3 | Lower importance feature(s) are experiencing minimal drift. No action needed. | Green  
4 | High importance feature(s) are experiencing minimal drift. No action needed, but monitor features that approach the threshold. | Green  
  
Note that points on the chart can also be gray or white. Gray circles
represent features that have been excluded from drift status calculation, and
white circles represent features set to high importance.

If you are the project owner, you can click the gear icon in the upper right
chart corner to reset the quadrants. By default, the drift threshold defaults
to .15. The Y-axis scales from 0 to the higher of 0.25 and the highest
observed drift value. These quadrants can be customized by [changing the drift
and importance thresholds](../deployment-settings/data-drift-settings.html).

## Feature Details chart¶

[![](../../images/drift-details.png)](../../images/drift-details.png)

The **Feature Details** chart provides a histogram that compares the
distribution of a selected feature in the training data to the distribution of
that feature in the inference data.

### Numeric features¶

For numeric data, DataRobot computes an efficient and precise approximation of
the distribution of each feature. Based on this, drift tracking is conducted
by comparing the normalized histogram for the training data to the scoring
data using the selected drift metrics.

The chart displays 13 bins for numeric features:

  * 10 bins capture the range of items observed in the training data.

  * Two bins capture _very high_ and _very low_ values—extreme values in the scoring data that fall outside the range of the training data. For example, to define the high and low value bins, the values are compared against the training data ranges, `min_training` and `max_training`. The low value bin contains values below the `min_training` range and the high value bin contains values above the `max_training` range.

  * One bin for the _missing_ count, containing all records with [missing feature values](../../reference/pred-ai-ref/model-ref.html#missing-values).

How are values added to the histogram bins?

The Data drift tab uses [Ben-Haim/Tom-Tov Centroid
Histograms](https://jmlr.org/papers/volume11/ben-haim10a/ben-haim10a.pdf).

### Categorical features¶

Unlike numeric data, where binning cutoffs for a histogram result from a data-
dependent calculation, categorical data is inherently discrete in form (that
is, not continuous), so binning is based on a defined category. Additionally,
there could be missing or unseen category levels in the scoring data.

The process for drift tracking of categorical features is to calculate the
fraction of rows for each categorical level ("bin") in the training data. This
results in a vector of percentages for each level. The 25 most frequent levels
are directly tracked—all other levels are aggregated to an **Other** bin. This
process is repeated for the scoring data, and the two vectors are compared
using the selected drift metric.

For categorical features, the chart includes two unique bins:

  * The **Other** bin contains all categorical features outside the 25 most frequent values. This aggregation is performed for drift tracking purposes; it doesn't represent the model's behavior.

  * The **New level** bin only displays after you make predictions with data that has a new value for a feature not in the training data. For example, consider a dataset about housing prices with the categorical feature `City`. If your inference data contains the value `Boston` and your training data did not, the `Boston` value (and other unseen cities) are represented in the New level bin.

To use the chart, select a feature from the dropdown. The list, which defaults
to the target feature, includes any of the features tracked. Click a point in
the **Feature Drift vs. Feature Importance** chart:

[![](../../images/data-drift-12.png)](../../images/data-drift-12.png)

### Text features¶

Text features are a high-cardinality problem, meaning the addition of new
words does not have the impact of, for example, new levels found in
categorical data. The method DataRobot uses to track drift of text features
accounts for the fact that writing is subjective and cultural and may have
spelling mistakes. In other words, to identify drift in text fields, it is
more important to identify a shift in the whole language rather than in
individual words.

Drift tracking for a text feature is conducted by:

  1. Detecting occurrences of the 1000 most frequent words from rows found in the training data.
  2. Calculating the fraction of rows that contain these terms for that feature in the training data and separately in the scoring data.
  3. Comparing the fraction in the scoring data to that in the training data.

The two vectors of occurrence fractions (one entry per word) are compared with
the available drift metrics. Prior to applying this methodology, DataRobot
performs basic tokenization by splitting the text feature into words (or
characters in the case of Japanese or Chinese).

For text features, the Feature Details chart replaces the feature drift bar
chart with a word cloud visualizing data distributions for each token and
revealing how much each individual token contributes to data drift in a
feature.

To access the feature drift word cloud for a text feature:

  1. Open the **Data Drift** tab of a [drift-enabled](../deployment-settings/data-drift-settings.html) deployment.

  2. On the **Summary** tab, in the **Feature Details** chart, select a text feature from dropdown list.

Note

You can also select a text feature from the **Selected Feature** dropdown list
in the **Data Drift** dashboard controls.

[![](../../images/drift-word-cloud.png)](../../images/drift-word-cloud.png)

  3. Use the dashboard controls to [configure the Data Drift dashboard](data-drift.html#configure-the-data-drift-dashboard).

  4. To interpret the feature drift word cloud for a text feature, you can hold the pointer over a token to view the following details:

Tip

When your pointer is over the word cloud, you can scroll up to zoom in and
view the text of smaller tokens.

[![](../../images/drift-word-cloud-details.png)](../../images/drift-word-
cloud-details.png)

Chart element | Description  
---|---  
Token | The tokenized text. Text size represents the token's drift contribution and text color represents the dataset prevalence. Stop words are hidden from this chart.  
Drift contribution | How much this particular token contributes to the feature's drift value, as reported in the [Feature Drift vs. Feature Importance](data-drift.html#feature-drift-vs-feature-importance-chart) and [Drift Over Time](data-drift.html#drift-over-time-chart) charts.  
Data distribution | How much more often this particular token appears in the training data or the predictions data. 
     * Blue: This token appears `X`% more often in training data.
     * Red: This token appears`X`% more often in predictions data.  

Note

Next to the **Export** button, you can click the settings icon
([![](../../images/icon-gear.png)](../../images/icon-gear.png)) and clear the
**Display text features as word cloud** check box to disable the feature drift
word cloud and view the standard chart:

[![](../../images/drift-word-cloud-disable.png)](../../images/drift-word-
cloud-disable.png)

## Drift Over Time chart¶

The **Drift Over Time** chart visualizes the difference in distribution over
time between the training dataset of the deployed model and the datasets used
to generate predictions in production. The drift away from the baseline
established with the training dataset is measured using the Population
Stability Index (PSI). As a model continues to make predictions on new data,
the change in the PSI over time is visualized for each tracked feature,
allowing you to identify [data
drift](../../reference/glossary/index.html#data-drift) trends.

As data drift can decrease your model's predictive power, determining when a
feature started drifting and monitoring how that drift changes (as your model
continues to make predictions on new data) can help you estimate the severity
of the issue. You can then compare data drift trends across the features in a
deployment to identify correlated drift trends between specific features. In
addition, the chart can help you identify seasonal effects (significant for
time-aware models). This information can help you identify the cause of data
drift in your deployed model, including data quality issues, changes in
feature composition, or changes in the context of the target variable. The
example below shows the PSI consistently increasing over time, indicating
worsening data drift for the selected feature.

The **Drift Over Time** chart includes the following elements and controls:

[![](../../images/drift-over-time.png)](../../images/drift-over-time.png)

| Chart element | Description  
---|---|---  
1 | Selected Feature | Selects a feature for drift over time analysis, which is then reported in the Drift Over Time chart and the Feature Details chart.  
2 | Time of Prediction / Sample size   
(X-axis) | Represents the time range of the predictions used to calculate the corresponding drift value (PSI). Below the X-axis, a bar chart represents the number of predictions made during the corresponding Time of Prediction. For more information on how time of prediction is represented in time series deployments, see the Time of prediction for time series deployments note.  
3 | Drift   
(Y-axis) | Represents the range of drift values (PSI) calculated for the corresponding Time of Prediction.  
4 | Training baseline | Represents the `0` PSI value of the training baseline dataset.  
5 | Drift status information | Displays the drift status and threshold information for the selected feature. Drift status visualizations are based on the [monitoring settings configured by the deployment owner](../deployment-settings/data-drift-settings.html). The deployment owner can also set the drift and importance thresholds in the Feature Drift vs Feature Importance chart settings.  
The possible drift status classifications are:

  * **Healthy (Green)** : The feature is experiencing minimal drift. No action needed, but monitor features that approach the threshold.
  * **At risk (Yellow)** : A lower importance feature is experiencing drift above the set threshold. Monitor closely.
  * **Failing (Red)** : A high importance feature is experiencing drift above the set threshold. Investigate immediately

Feature importance is determined by comparing the feature impact score with
the importance threshold value. For an important feature, the feature impact
score is greater than or equal to the importance threshold.  
6 | Export | Exports the Drift Over Time chart.  
Time of prediction for time series deployments

The default prediction timestamp method for time series deployments is
forecast date (i.e., forecast point + forecast distance), not the time of the
prediction request. Forecast date allows a common time axis to be used between
the training data and the basis of data drift and accuracy statistics. For
example, using forecast date, if the prediction data has dates from June 1 to
June 10, the forecast point is set to June 10, and the forecast distance is
set to `+1 - + 7` days, predictions are available and data drift is tracked
for June 11 - 17.

You can select from the following [prediction timestamp](../deployment/deploy-
methods/add-deploy-info.html#prediction-history-and-service-health) options
when deploying a model:

  * **Use value from date/time feature** : _Default_. Use the date/time provided as a feature with the prediction data (e.g., forecast date) to determine the timestamp.
  * **Use time of prediction request** : Use the time you _submitted_ the prediction request to determine the timestamp.

To view additional information on the **Drift Over Time** chart, hover over a
marker in the chart to see the **Time of Prediction** , **PSI** , and **Sample
size** :

[![](../../images/drift-over-time-details.png)](../../images/drift-over-time-
details.png)

Tip

The X-axis of the Drift Over Time chart aligns with the X-axis of the
Predictions Over Time chart below to make comparing the two charts easier. In
addition, the _Sample size_ data on the Drift Over Time chart is equivalent to
the _Number of Predictions_ data from the Predictions Over Time chart.

## Predictions Over Time chart¶

The **Predictions Over Time** chart provides an at-a-glance determination of
how the model's predictions have changed over time. For example:

> Dave sees that his model is predicting `1` (readmitted) noticeably more
> frequently over the past month. Because he doesn't know of a corresponding
> change in the actual distribution of readmissions, he suspects that the
> model has become less accurate. With this information, he investigates
> further whether he should consider retraining.

Although the charts for binary classification and regression differ slightly,
the takeaway is the same—are the plot lines relatively stable across time? If
not, is there a business reason for the anomaly (for example, a blizzard)? One
way to check this is to look at the bar chart below the plot. If the point for
a binned period is abnormally high or low, check the histogram below to ensure
there are enough predictions for this to be a reliable data point.

Time of Prediction

The Time of Prediction value differs between the [**Data drift**](data-
drift.html) and [**Accuracy**](deploy-accuracy.html) tabs and the [**Service
health**](service-health.html) tab:

  * On the **Service health** tab, the "time of prediction request" is _always_ the time the prediction server _received_ the prediction request. This method of prediction request tracking accurately represents the prediction service's health for diagnostic purposes. 

  * On the **Data drift** and **Accuracy** tabs, the "time of prediction request" is, _by default_ , the time you _submitted_ the prediction request, which you can override with the prediction timestamp in the [Prediction History and Service Health](../deployment/deploy-methods/add-deploy-info.html#prediction-history-and-service-health) settings.

Additionally, both charts have `Training` and `Scoring` labels across the
X-axis. The `Training` label indicates the section of the chart that shows the
distribution of predictions made on the holdout set of training data for the
model. It will always have one point on the chart. The `Scoring` label
indicates the section of the chart showing the distribution of predictions
made on the deployed model. `Scoring` indicates that the model is in use to
make predictions. It will have multiple points along the chart to indicate how
prediction distributions change over time.

### For regression projects¶

The **Predictions Over Time** chart for regression projects plots the average
predicted value, as well as a visual indicator of the middle 80% range of
predicted values for both training and prediction data. If training data is
uploaded, the graph displays both the 10th-90th percentile and the mean value
of the target ([![](../../images/icon-mean.png)](../../images/icon-mean.png)).

[![](../../images/data-drift-2.png)](../../images/data-drift-2.png)

Hover over a point on the chart to view its details:

[![](../../images/data-drift-4.png)](../../images/data-drift-4.png)

  * **Date** : The starting date of the bin data. Displayed values are based on counts from this date to the next point along the graph. For example, if the date on point A is 01-07 and point B is 01-14, then point A covers everything from 01-07 to 01-13 (inclusive).
  * **Average Predicted Value** : For all points included in the bin, this is the average of their values.
  * **Predictions** : The number of predictions included in the bin. Compare this value against other points if you suspect anomalous data.
  * **10th-90th Percentile** : Percentile of predictions for that time period.

Note that you can also display this information for the mean value of the
target by hovering on the point in the training data.

### For binary classification projects¶

The **Predictions Over Time** chart for binary classification projects plots
the class percentages based on the labels you set when you [added the
deployment](../deployment/deploy-methods/index.html) (in this example, `0` and
`1`). It also reports the threshold set for prediction output. The threshold
is set when adding your deployment to the inventory and cannot be revised.

[![](../../images/data-drift-3.png)](../../images/data-drift-3.png)

Hover over a point on the chart to view its details:

[![](../../images/data-drift-8.png)](../../images/data-drift-8.png)

  * **Date** : The starting date of the bin data. Displayed values are based on counts from this date to the next point along the graph. For example, if the date on point A is 01-07 and point B is 01-14, then point A covers everything from 01-07 to 01-13 (inclusive).
  * **< class-label>**: For all points included in the bin, the percentage of those in the "positive" class (`0` in this example).
  * **< class-label>**: For all points included in the bin, the percentage of those in the "negative" class (`1` in this example).
  * **Number of Predictions** : The number of predictions included in the bin. Compare this value against other points if you suspect anomalous data.

Additionally, the chart displays the mean value of the target in the training
data. As with all plotted points, you can hover over it to see the specific
values.

[![](../../images/data-drift-11.png)](../../images/data-drift-11.png)

The chart also includes a toggle in the upper-right corner that allows you to
switch between continuous and binary modes (only for binary classification
deployments):

[![](../../images/data-drift-16.png)](../../images/data-drift-16.png)

Continuous mode shows the positive class predictions as probabilities between
0 and 1, without taking the prediction threshold into account:

[![](../../images/data-drift-17.png)](../../images/data-drift-17.png)

Binary mode takes the prediction threshold into account and shows, of all
predictions made, the percentage for each possible class:

[![](../../images/data-drift-18.png)](../../images/data-drift-18.png)

### Prediction warnings integration¶

If you have enabled [prediction warnings](../deployment-settings/humility-
settings.html#prediction-warnings) for a deployment, any anomalous prediction
values that trigger a warning are flagged in the Predictions Over Time bar
chart.

Prediction warnings availability

Prediction warnings are only available for deployments using regression
models. This feature does not support classification or time series models.

The yellow section of the bar chart represents the anomalous predictions for a
point in time.

[![](../../images/dd-warning-1.png)](../../images/dd-warning-1.png)

To view the number of anomalous predictions for a specific time period, hover
over the point on the plot corresponding to the flagged predictions in the bar
chart.

[![](../../images/dd-warning-2.png)](../../images/dd-warning-2.png)

## Use the version selector¶

[![](../../images/data-drift-20.png)](../../images/data-drift-20.png)

You can change the data drift display to analyze the current, or any previous,
version of a model in the deployment. Initially, if there has been no model
replacement, you only see the **Current** option. The models listed in the
dropdown can also be found in the **History** section of the
[**Overview**](dep-overview.html) tab. This functionality is only supported
with deployments made with models or model images.

## Use the time range and resolution dropdowns¶

The **Range** and **Resolution** dropdowns help diagnose deployment issues by
allowing you to change the granularity of the three deployment monitoring
tabs: **Data Drift** , [**Service Health**](service-health.html), and
[**Accuracy**](deploy-accuracy.html).

Expand the **Range** dropdown (1) to select the start and end dates for the
time range you want to examine. You can specify the time of day for each date
(to the nearest hour, rounded down) by editing the value after selecting a
date. When you have determined the desired time range, click **Update range**
(2). Select the **Range** reset icon ([![](../../images/icon-
refresh.png)](../../images/icon-refresh.png)) (3) to restore the time range to
the previous setting.

[![](../../images/dd-range-1.png)](../../images/dd-range-1.png)

Note

The date picker only allows you to select dates and times between the start
date of the deployment's current version of a model and the current date.

After setting the time range, use the **Resolution** dropdown to determine the
granularity of the date slider. Select from hourly, daily, weekly, and monthly
granularity based on the time range selected. The following **Resolution**
settings are available, based on the selected range:

Resolution | Selected range requirement  
---|---  
Hourly | Less than 7 days.  
Daily | Between 1-60 days (inclusive).  
Weekly | Between 1-52 weeks (inclusive).  
Monthly | At least 1 month and less than 120 months.  
  
When you choose a new value from the **Resolution** dropdown, the resolution
of the date selection slider changes. Then, you can select start and end
points on the slider to hone in on the time range of interest.

[![](../../images/data-drift-13.png)](../../images/data-drift-13.png)

Note that the selected slider range also carries across the [**Service
Health**](service-health.html) and [**Accuracy**](deploy-accuracy.html) tabs
(but not across deployments).

## Use the date slider¶

The date slider limits the time range used for comparing prediction data to
training data. The upper dates displayed in the slider, left and right edges,
indicate the range currently used for comparison in the page's visualizations.
The lower dates, left and right edges, indicate the full date range of
prediction data available. The circles mark the "data buckets," which are
determined by the time range.

[![](../../images/data-drift-9.png)](../../images/data-drift-9.png)

To use the slider, click a point to move the line or drag the endpoint left or
right.

[![](../../images/data-drift-10.png)](../../images/data-drift-10.png)

The visualizations use predictions from the starting point of the updated time
range as the baseline reference point, comparing them to predictions occurring
up to the last date of the selected time range.

You can also move the slider to a different time interval while maintaining
the periodicity. Click anywhere on the slider between the two endpoints to
drag it (you will see a hand icon on your cursor).

[![](../../images/data-drift-14.png)](../../images/data-drift-14.png)

In the example above, you see the slider spans a 3-month time interval. You
can drag the slider and maintain the time interval of 3 months for different
dates.

[![](../../images/data-drift-15.png)](../../images/data-drift-15.png)

By default, the slider is set to display the same date range that is used to
calculate and display drift status. For example, if drift status captures the
last week, then the default slider range will span from the last week to the
current date.

You can move the slider to any date range without affecting the data drift
status display on the health dashboard. If you do so, a **Reset** button
appears above the slider. Clicking it will revert the slider to the default
date range that matches the range of the drift status.

## Use the class selector¶

Multiclass deployments offer [class-based configuration](deploy-
accuracy.html#class-selector) to modify the data displayed on the Data Drift
graphs.

Predictions over Time multiclass graph:

[![](../../images/multi-dep-6.png)](../../images/multi-dep-6.png)

Feature Details multiclass graph:

[![](../../images/multi-dep-7.png)](../../images/multi-dep-7.png)

## Drill down on the Data Drift tab¶

The **Data Drift** > **Drill Down** chart visualizes the difference in
distribution over time between the training dataset of the deployed model and
the datasets used to generate predictions in production. The drift away from
the baseline established with the training dataset is measured using the
Population Stability Index (PSI). As a model continues to make predictions on
new data, the change in the drift status over time is visualized as a heat map
for each tracked feature, allowing you to identify [data
drift](../../reference/glossary/index.html#data-drift) trends.

Because data drift can decrease your model's predictive power, determining
when a feature started drifting and monitoring how that drift changes (as your
model continues to make predictions on new data) can help you estimate the
severity of the issue. Using the **Drill Down** tab, you can compare data
drift heat maps across the features in a deployment to identify correlated
drift trends. In addition, you can select one or more features from the heat
map to view a **Feature Drift Comparison** chart, comparing the change in a
feature's data distribution between a reference time period and a comparison
time period to visualize drift. This information helps you identify the cause
of data drift in your deployed model, including data quality issues, changes
in feature composition, or changes in the context of the target variable.

To access the **Drill Down** tab:

  1. Click **Deployments** , and then select a [drift-enabled](../deployment-settings/data-drift-settings.html) deployment from the **Deployments** inventory.

  2. In the deployment, click **Data Drift** , and then click **Drill Down** :

[![](../../images/pp-drill-down-location.png)](../../images/pp-drill-down-
location.png)

  3. On the **Drill Down** tab:

     * Configure the display settings.

     * Use the feature drift heat map.

     * Use the feature drift comparison chart.

### Configure the drill down display settings¶

The **Drill Down** tab includes the following display controls:

[![](../../images/pp-drill-down-controls.png)](../../images/pp-drill-down-
controls.png)

| Control | Description  
---|---|---  
1 | Model | Updates the heatmap to display the model you selected from the dropdown.  
2 | Date slider | Limits the range of data displayed on the dashboard (i.e., zooms in on a specific time period).  
3 | Range (UTC) | Sets the date range displayed for the deployment date slider.  
4 | Resolution | Sets the time granularity of the deployment date slider.  
5 | Reset | Reverts the dashboard controls to the default settings.  
  
### Use the feature drift heat map¶

The **Feature Drift for all features** heat map includes the following
elements and controls:

[![](../../images/pp-drill-down-heatmap.png)](../../images/pp-drill-down-
heatmap.png)

| Element | Description  
---|---|---  
1 | Prediction time   
(X-axis) | Represents the time range of the predictions used to calculate the corresponding drift value (PSI). Below the X-axis, the **Prediction sample size** bar chart represents the number of predictions made during the corresponding prediction time range.  
2 | Feature   
(Y-axis) | Represents the features in a deployment's dataset. Click a feature name to generate the feature drift comparison below.  
3 | Status heat map | Displays the drift status over time for each of a deployment's features. Drift status visualizations are based on the [monitoring settings configured by the deployment owner](../deployment-settings/data-drift-settings.html). The deployment owner can also set the drift and importance thresholds in the [Feature Drift vs Feature Importance chart settings](data-drift.html#feature-drift-vs-feature-importance-chart).  
The possible drift status classifications are:

  * **Healthy (Green)** : The feature is experiencing minimal drift. No action needed, but monitor features that approach the threshold.
  * **At risk (Yellow)** : A lower importance feature is experiencing drift above the set threshold. Monitor closely.
  * **Failing (Red)** : A high importance feature is experiencing drift above the set threshold. Investigate immediately

Feature importance is determined by comparing the feature impact score with
the importance threshold value. For an important feature, the feature impact
score is greater than or equal to the importance threshold.  
4 | Prediction sample size | Displays the number of rows of prediction data used to calculate the data drift for the given time period. To view additional information on the prediction sample size, hover over a bin in the chart to see the time of prediction range and the sample size value.  
  
### Use the feature drift comparison chart¶

The **Feature Drift Comparison** section includes the following elements and
controls:

[![](../../images/pp-drill-down-feature-compare.png)](../../images/pp-drill-
down-feature-compare.png)

| Element | Description  
---|---|---  
1 | Reference period | Sets the date range of the period to use as a baseline for the drift comparison charts.  
2 | Comparison period | Sets the date range of the period to compare data distribution against the reference period. You can also select an area of interest on the heat map to serve as the comparison period.  
3 | Feature values   
(X-axis) | Represents the range of values in the dataset for the feature in the Feature Drift Comparison chart.  
4 | Percentage of Records   
(y-axis) | Represents the percentage of the total dataset represented by a range of values and provides a visual comparison between the selected reference and comparison periods.  
5 | Add a feature drift comparison chart | Generates a Feature Drift Comparison chart for a selected feature.  
6 | Remove this chart | Removes a Feature Drift Comparison chart.  
Set the comparison period on the feature drift heat map

To select an area of interest on the heat map to serve as the comparison
period, click and drag to select the period you want to target for feature
drift comparison:

[![](../../images/pp-drift-comparison-period.gif)](../../images/pp-drift-
comparison-period.gif)

To view additional information on a **Feature Drift Comparison** chart, hover
over a bar in the chart to see the range of values contained in that bar, the
percentage of the total dataset those values represent in the **Reference
period** , and the percentage of the total dataset those values represent in
the **Comparison period** :

[![](../../images/pp-drill-down-feature-compare-detail.png)](../../images/pp-
drill-down-feature-compare-detail.png)

Back to top

