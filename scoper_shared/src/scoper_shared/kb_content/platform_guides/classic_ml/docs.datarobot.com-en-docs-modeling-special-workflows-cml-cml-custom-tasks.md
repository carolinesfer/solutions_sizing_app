[ DataRobot docs ](../../../index.html "DataRobot docs")

  * [ Data ](../../../data/index.html) [ Data ](../../../data/index.html)
    * [ Data connections ](../../../data/connect-data/index.html) [ Data connections ](../../../data/connect-data/index.html)
      * [ Share secure configurations ](../../../data/connect-data/secure-config.html)
      * [ Data connections ](../../../data/connect-data/data-conn.html)
    * [ AI Catalog ](../../../data/ai-catalog/index.html) [ AI Catalog ](../../../data/ai-catalog/index.html)
      * [ Load data ](../../../data/ai-catalog/catalog.html)
      * [ Manage assets ](../../../data/ai-catalog/manage-asset.html)
      * [ Work with assets ](../../../data/ai-catalog/catalog-asset.html)
      * [ Schedule snapshots ](../../../data/ai-catalog/snapshot.html)
      * [ Prepare data with Spark SQL ](../../../data/ai-catalog/spark.html)
    * [ Import data ](../../../data/import-data/index.html) [ Import data ](../../../data/import-data/index.html)
      * [ Import to DataRobot directly ](../../../data/import-data/import-to-dr.html)
      * [ Large datasets ](../../../data/import-data/large-data/index.html) [ Large datasets ](../../../data/import-data/large-data/index.html)
        * [ Fast EDA for large datasets ](../../../data/import-data/large-data/fast-eda.html)
    * [ Transform data ](../../../data/transform-data/index.html) [ Transform data ](../../../data/transform-data/index.html)
      * [ Interaction-based transformations ](../../../data/transform-data/feature-disc.html)
      * [ Feature Discovery ](../../../data/transform-data/feature-discovery/index.html) [ Feature Discovery ](../../../data/transform-data/feature-discovery/index.html)
        * [ End-to-end Feature Discovery ](../../../data/transform-data/feature-discovery/enrich-data-using-feature-discovery.html)
        * [ Set up Feature Discovery projects ](../../../data/transform-data/feature-discovery/fd-overview.html)
        * [ Snowflake integration ](../../../data/transform-data/feature-discovery/fd-snowflake.html)
        * [ Feature Discovery settings ](../../../data/transform-data/feature-discovery/fd-adv-opt.html)
        * [ Time-aware feature engineering ](../../../data/transform-data/feature-discovery/fd-time.html)
        * [ Derived features ](../../../data/transform-data/feature-discovery/fd-gen.html)
        * [ Predictions ](../../../data/transform-data/feature-discovery/fd-predict.html)
      * [ Manual transformations ](../../../data/transform-data/feature-transforms.html)
    * [ Analyze data ](../../../data/analyze-data/index.html) [ Analyze data ](../../../data/analyze-data/index.html)
      * [ Assess data quality with EDA ](../../../data/analyze-data/assess-data-quality-eda.html)
      * [ Analyze features using histograms ](../../../data/analyze-data/analyze-histogram.html)
      * [ Analyze frequent values ](../../../data/analyze-data/analyze-frequent-values.html)
      * [ Feature details ](../../../data/analyze-data/histogram.html)
      * [ Exploratory Spatial Data Analysis (ESDA) ](../../../data/analyze-data/lai-esda.html)
      * [ Feature Associations ](../../../data/analyze-data/feature-assoc.html)
      * [ Use data pipelines for ingest and transformation ](../../../data/analyze-data/pipelines.html)
    * [ Data preview features ](../../../data/data-preview/index.html) [ Data preview features ](../../../data/data-preview/index.html)
      * [ Create feature lists in the Relationship Editor ](../../../data/data-preview/safer-rel-editor-feature-lists.html)
    * [ Data FAQ ](../../../data/data-faq.html)
  * [ Modeling ](../../index.html) [ Modeling ](../../index.html)
    * [ Build models ](../../build-models/index.html) [ Build models ](../../build-models/index.html)
      * [ Build models ](../../build-models/build-basic/index.html) [ Build models ](../../build-models/build-basic/index.html)
        * [ Basic model workflow ](../../build-models/build-basic/model-data.html)
        * [ Work with feature lists ](../../build-models/build-basic/feature-lists.html)
        * [ Unlock Holdout ](../../build-models/build-basic/unlocking-holdout.html)
        * [ Comprehensive Autopilot ](../../build-models/build-basic/more-accuracy.html)
        * [ Add/delete models ](../../build-models/build-basic/creating-addl-models.html)
        * [ Frozen runs ](../../build-models/build-basic/frozen-run.html)
        * [ Model Repository ](../../build-models/build-basic/repository.html)
      * [ Advanced options ](../../build-models/adv-opt/index.html) [ Advanced options ](../../build-models/adv-opt/index.html)
        * [ Additional ](../../build-models/adv-opt/additional.html)
        * [ Bias and Fairness ](../../build-models/adv-opt/fairness-metrics.html)
        * [ Clustering advanced options ](../../build-models/adv-opt/time-series-cluster-adv-opt.html)
        * [ External Predictions ](../../build-models/adv-opt/external-preds.html)
        * [ Feature Constraints ](../../build-models/adv-opt/feature-con.html)
        * [ Image Augmentation ](../../build-models/adv-opt/ttia.html)
        * [ Partitioning ](../../build-models/adv-opt/partitioning.html)
        * [ Smart Downsampling ](../../build-models/adv-opt/smart-ds.html)
        * [ Time series ](../../build-models/adv-opt/time-series-adv-opt.html)
        * [ GPUs for deep learning ](../../build-models/adv-opt/gpus.html)
    * [ Model insights ](../../analyze-models/index.html) [ Model insights ](../../analyze-models/index.html)
      * [ Evaluate ](../../analyze-models/evaluate/index.html) [ Evaluate ](../../analyze-models/evaluate/index.html)
        * [ Accuracy Over Space ](../location-ai/lai-insights.html)
        * [ Accuracy Over Time ](../../analyze-models/evaluate/aot.html)
        * [ Advanced Tuning ](../../analyze-models/evaluate/adv-tuning.html)
        * [ Anomaly visualizations ](../../analyze-models/evaluate/anom-viz.html)
        * [ Confusion Matrix (for multiclass models) ](../../analyze-models/evaluate/multiclass.html)
        * [ Forecasting Accuracy ](../../analyze-models/evaluate/forecast-acc.html)
        * [ Forecast vs Actual ](../../analyze-models/evaluate/fore-act.html)
        * [ Lift Chart ](../../analyze-models/evaluate/lift-chart.html)
        * [ Period Accuracy ](../../analyze-models/evaluate/period-accuracy.html)
        * [ Residuals ](../../analyze-models/evaluate/residuals.html)
        * [ ROC Curve tools ](../../analyze-models/evaluate/roc-curve-tab/index.html) [ ROC Curve tools ](../../analyze-models/evaluate/roc-curve-tab/index.html)
          * [ Use the ROC Curve tools ](../../analyze-models/evaluate/roc-curve-tab/roc-curve-tab-use.html)
          * [ Select data and display threshold ](../../analyze-models/evaluate/roc-curve-tab/threshold.html)
          * [ Confusion matrix ](../../analyze-models/evaluate/roc-curve-tab/confusion-matrix.html)
          * [ Prediction Distribution graph ](../../analyze-models/evaluate/roc-curve-tab/pred-dist-graph.html)
          * [ ROC curve ](../../analyze-models/evaluate/roc-curve-tab/roc-curve.html)
          * [ Profit curve ](../../analyze-models/evaluate/roc-curve-tab/profit-curve.html)
          * [ Cumulative Charts ](../../analyze-models/evaluate/roc-curve-tab/cumulative-charts.html)
          * [ Custom charts ](../../analyze-models/evaluate/roc-curve-tab/custom-charts.html)
          * [ Metrics ](../../analyze-models/evaluate/roc-curve-tab/metrics.html)
        * [ Series Insights (clustering) ](../../analyze-models/evaluate/series-insights.html)
        * [ Series Insights (multiseries) ](../../analyze-models/evaluate/series-insights-multi.html)
        * [ Stability ](../../analyze-models/evaluate/stability.html)
        * [ Training Dashboard ](../../analyze-models/evaluate/training-dash.html)
      * [ Understand ](../../analyze-models/understand/index.html) [ Understand ](../../analyze-models/understand/index.html)
        * [ Cluster Insights ](../../analyze-models/understand/cluster-insights.html)
        * [ Feature Effects ](../../analyze-models/understand/feature-effects.html)
        * [ Feature Impact ](../../analyze-models/understand/feature-impact.html)
        * [ Prediction Explanations ](../../analyze-models/understand/pred-explain/index.html) [ Prediction Explanations ](../../analyze-models/understand/pred-explain/index.html)
          * [ Prediction Explanations overview ](../../analyze-models/understand/pred-explain/predex-overview.html)
          * [ SHAP Prediction Explanations ](../../analyze-models/understand/pred-explain/shap-pe.html)
          * [ XEMP Prediction Explanations ](../../analyze-models/understand/pred-explain/xemp-pe.html)
          * [ Text Prediction Explanations ](../../analyze-models/understand/pred-explain/predex-text.html)
          * [ Prediction Explanations for clusters ](../../analyze-models/understand/pred-explain/cluster-pe.html)
          * [ Prediction Explanations for time-aware projects ](../../analyze-models/understand/pred-explain/ts-otv-predex.html)
        * [ Word Cloud ](../../analyze-models/understand/word-cloud.html)
      * [ Describe ](../../analyze-models/describe/index.html) [ Describe ](../../analyze-models/describe/index.html)
        * [ Blueprint ](../../analyze-models/describe/blueprints.html)
        * [ Blueprint JSON ](../../analyze-models/describe/blueprint-json.html)
        * [ Coefficients (preprocessing) ](../../analyze-models/describe/coefficients.html)
        * [ Constraints (monotonic) ](../../analyze-models/describe/monotonic.html)
        * [ Data Quality Handling Report ](../../analyze-models/describe/dq-report.html)
        * [ Eureqa Models ](../../analyze-models/describe/eureqa.html)
        * [ Log ](../../analyze-models/describe/log.html)
        * [ Model Info ](../../analyze-models/describe/model-info.html)
        * [ Rating Tables ](../../analyze-models/describe/rating-table.html)
        * [ GA2M output (from Rating Tables) ](../../analyze-models/describe/ga2m.html)
      * [ Predict ](../../analyze-models/predictions/index.html) [ Predict ](../../analyze-models/predictions/index.html)
        * [ Deploy ](../../analyze-models/predictions/deploy.html)
        * [ Downloads ](../../analyze-models/predictions/download.html)
        * [ Make Predictions ](../../analyze-models/predictions/predict.html)
        * [ Portable Predictions ](../../analyze-models/predictions/port-pred.html)
      * [ Compliance ](../../analyze-models/compliance/index.html) [ Compliance ](../../analyze-models/compliance/index.html)
        * [ Model Compliance ](../../analyze-models/compliance/compliance.html)
        * [ Template Builder for compliance reports ](../../analyze-models/compliance/template-builder.html)
      * [ Comments ](../../analyze-models/comments/index.html)
      * [ Bias and Fairness ](../../analyze-models/bias/index.html) [ Bias and Fairness ](../../analyze-models/bias/index.html)
        * [ Cross-Class Accuracy ](../../analyze-models/bias/cross-acc.html)
        * [ Cross-Class Data Disparity ](../../analyze-models/bias/cross-data.html)
        * [ Per-Class Bias ](../../analyze-models/bias/per-class.html)
      * [ Other ](../../analyze-models/other/index.html) [ Other ](../../analyze-models/other/index.html)
        * [ Bias vs Accuracy ](../../analyze-models/other/bias-tab.html)
        * [ Insights ](../../analyze-models/other/analyze-insights.html)
        * [ Learning Curves ](../../analyze-models/other/learn-curve.html)
        * [ Model Comparison ](../../analyze-models/other/model-compare.html)
        * [ Speed vs Accuracy ](../../analyze-models/other/speed.html)
    * [ Specialized workflows ](../index.html) [ Specialized workflows ](../index.html)
      * [ Bias and Fairness resources ](../bias-resources.html)
      * [ Composable ML ](index.html) [ Composable ML ](index.html)
        * [ Composable ML overview ](cml-overview.html)
        * [ Composable ML Quickstart ](cml-quickstart.html)
        * [ Modify a blueprint ](cml-blueprint-edit.html)
        * [ Create custom tasks ](cml-custom-tasks.html) [ Create custom tasks ](cml-custom-tasks.html)

[For Self-Managed AI Platform users running v11.1, see the on-premise platform
documentation __](/11.1/en/docs/index.html)

Create custom tasks

          * Understand custom tasks 
            * Components of a custom task 
            * Task types 
            * Use a custom task 
            * Understand task content 
              * custom.py/custom.R 
              * model-metadata.yaml 
              * requirements.txt 
          * Define task code 
            * init() 
              * init() example 
              * init() input 
              * init() output 
            * fit() 
              * fit() examples 
              * How fit() works 
              * How to use fit() 
              * fit() input parameters 
              * fit() output 
            * load_model() 
              * load_model() example 
              * load_model() input 
              * load_model() output 
            * predict() 
              * predict() examples 
              * predict() input 
              * predict() output 
            * predict_proba() 
              * predict_proba() examples 
              * predict_proba() input 
              * predict_proba() output 
            * transform() 
              * transform() example 
              * transform() input 
              * transform() output 
          * Define task metadata 
          * Define the task environment 
          * Test the task locally 
            * Prerequisites 
            * Test compatibility with DataRobot 
            * Test task logic 
          * Upload the task 
            * Updating code 
          * Compose and train a blueprint 
            * Single-task blueprint 
            * Multitask blueprint 
          * Get insights 
            * Built-in insights 
            * Custom insights 
          * Deploy 
          * Download training artifacts 
          * Implicit sharing 

        * [ Custom environments ](cml-custom-env.html)
        * [ DRUM CLI tool ](cml-drum.html)
        * [ Enable network access for custom tasks ](custom-task-network-access.html)
      * [ Document AI ](../doc-ai/index.html) [ Document AI ](../doc-ai/index.html)
        * [ Document AI overview ](../doc-ai/doc-ai-overview.html)
        * [ Document ingest and modeling ](../doc-ai/doc-ai-ingest.html)
        * [ Document AI insights ](../doc-ai/doc-ai-insights.html)
        * [ Predictions from documents ](../doc-ai/doc-ai-predictions.html)
      * [ Location AI ](../location-ai/index.html) [ Location AI ](../location-ai/index.html)
        * [ Data ingest ](../location-ai/lai-ingest.html)
        * [ Exploratory Spatial Data Analysis (ESDA) ](../location-ai/lai-esda.html)
        * [ Modeling ](../location-ai/lai-model.html)
        * [ Accuracy Over Space ](../location-ai/lai-insights.html)
      * [ Unsupervised learning ](../unsupervised/index.html) [ Unsupervised learning ](../unsupervised/index.html)
        * [ Anomaly detection ](../unsupervised/anomaly-detection.html)
        * [ Clustering ](../unsupervised/clustering.html)
      * [ Visual AI ](../visual-ai/index.html) [ Visual AI ](../visual-ai/index.html)
        * [ Visual AI overview ](../visual-ai/vai-overview.html)
        * [ Build Visual AI models ](../visual-ai/vai-model.html)
        * [ Train-time image augmentation ](../visual-ai/tti-augment/index.html) [ Train-time image augmentation ](../visual-ai/tti-augment/index.html)
          * [ About augmented models ](../visual-ai/tti-augment/ttia-introduction.html)
          * [ Transformations and lists ](../visual-ai/tti-augment/ttia-lists.html)
          * [ Use case examples ](../visual-ai/tti-augment/ttia-examples.html)
        * [ Model insights ](../visual-ai/vai-insights.html)
        * [ Tune models ](../visual-ai/vai-tuning.html)
        * [ Visual AI predictions ](../visual-ai/vai-predictions.html)
      * [ Multilabel modeling ](../multilabel.html)
      * [ Out-of-time validation modeling ](../otv.html)
      * [ Text AI resources ](../textai-resources.html)
    * [ Time-series modeling ](../../time/index.html) [ Time-series modeling ](../../time/index.html)
      * [ What is time-aware modeling? ](../../time/whatis-time.html)
      * [ Time series modeling ](../../time/ts-flow-overview.html)
      * [ Time series insights ](../../time/ts-leaderboard.html)
      * [ Time series predictions ](../../time/ts-predictions.html)
      * [ Time series portable predictions with prediction intervals ](../../time/ts-port-pred-intervals.html)
      * [ Multiseries modeling ](../../time/multiseries.html)
      * [ Clustering ](../../time/ts-clustering.html)
      * [ Segmented modeling ](../../time/ts-segmented.html)
      * [ Nowcasting ](../../time/nowcasting.html)
      * [ External prediction comparison ](../../time/cyob.html)
      * [ Batch predictions for TTS and LSTM models ](../../time/ts-tts-lstm-batch-pred.html)
      * [ Time series advanced modeling ](../../time/ts-adv-modeling/index.html) [ Time series advanced modeling ](../../time/ts-adv-modeling/index.html)
        * [ Time series advanced options ](../../time/ts-adv-modeling/ts-adv-opt.html)
        * [ Clustering advanced options ](../../time/ts-adv-modeling/ts-cluster-adv-opt.html)
        * [ Date/time partitioning advanced options ](../../time/ts-adv-modeling/ts-date-time.html)
        * [ Customizing time series projects ](../../time/ts-adv-modeling/ts-customization.html)
      * [ Time series modeling data ](../../time/ts-modeling-data/index.html) [ Time series modeling data ](../../time/ts-modeling-data/index.html)
        * [ Create the modeling dataset ](../../time/ts-modeling-data/ts-create-data.html)
        * [ Data prep for time series ](../../time/ts-modeling-data/ts-data-prep.html)
        * [ Restore features removed by reduction ](../../time/ts-modeling-data/restore-features.html)
    * [ AutoML preview features ](../../automl-preview/index.html) [ AutoML preview features ](../../automl-preview/index.html)
      * [ Quantile regression analysis ](../../automl-preview/quantile-reg.html)
      * [ Configure hyperparameters for custom tasks ](../../automl-preview/cml-hyperparam.html)
    * [ Modeling FAQ ](../../general-modeling-faq.html)
    * [ Value Tracker ](../../value-tracker.html)
    * [ Project control center ](../../manage-projects.html)
  * [ Predictions ](../../../predictions/index.html) [ Predictions ](../../../predictions/index.html)
    * [ Real-time scoring methods ](../../../predictions/realtime/index.html) [ Real-time scoring methods ](../../../predictions/realtime/index.html)
      * [ Prediction API snippets ](../../../predictions/realtime/code-py.html)
      * [ Qlik predictions ](../../../predictions/realtime/integration-code-snippets.html)
    * [ Batch prediction methods ](../../../predictions/batch/index.html) [ Batch prediction methods ](../../../predictions/batch/index.html)
      * [ Batch prediction UI ](../../../predictions/batch/batch-dep/index.html) [ Batch prediction UI ](../../../predictions/batch/batch-dep/index.html)
        * [ Make a one-time batch prediction ](../../../predictions/batch/batch-dep/batch-pred.html)
        * [ Schedule recurring batch prediction jobs ](../../../predictions/batch/batch-dep/batch-pred-jobs.html)
        * [ Manage prediction job definitions ](../../../predictions/batch/batch-dep/manage-pred-job-def.html)
        * [ Snowflake prediction job examples ](../../../predictions/batch/batch-dep/pred-job-examples-snowflake.html)
      * [ Prediction monitoring jobs ](../../../predictions/batch/pred-monitoring-jobs/index.html) [ Prediction monitoring jobs ](../../../predictions/batch/pred-monitoring-jobs/index.html)
        * [ Create monitoring jobs ](../../../predictions/batch/pred-monitoring-jobs/ui-monitoring-jobs.html)
        * [ Monitoring jobs API ](../../../predictions/batch/pred-monitoring-jobs/api-monitoring-jobs.html)
        * [ Manage monitoring job definitions ](../../../predictions/batch/pred-monitoring-jobs/manage-monitoring-job-def.html)
      * [ Manage batch jobs ](../../../predictions/batch/batch-jobs.html)
      * [ Batch prediction scripts ](../../../predictions/batch/cli-scripts.html)
    * [ Portable prediction methods ](../../../predictions/port-pred/index.html) [ Portable prediction methods ](../../../predictions/port-pred/index.html)
      * [ Scoring Code ](../../../predictions/port-pred/scoring-code/index.html) [ Scoring Code ](../../../predictions/port-pred/scoring-code/index.html)
        * [ Download Scoring Code from the Leaderboard ](../../../predictions/port-pred/scoring-code/sc-download-leaderboard.html)
        * [ Download Scoring Code from a deployment ](../../../predictions/port-pred/scoring-code/sc-download-deployment.html)
        * [ Download Scoring Code from the Leaderboard (Legacy) ](../../../predictions/port-pred/scoring-code/sc-download-legacy.html)
        * [ Scoring Code for time series projects ](../../../predictions/port-pred/scoring-code/sc-time-series.html)
        * [ Scoring at the command line ](../../../predictions/port-pred/scoring-code/scoring-cli.html)
        * [ Scoring Code usage examples ](../../../predictions/port-pred/scoring-code/quickstart-api.html)
        * [ JAR structure ](../../../predictions/port-pred/scoring-code/jar-package.html)
        * [ Generate Java models in an existing project ](../../../predictions/port-pred/scoring-code/build-verify.html)
        * [ Backward-compatible Java API ](../../../predictions/port-pred/scoring-code/java-back-compat.html)
        * [ Scoring Code JAR integrations ](../../../predictions/port-pred/scoring-code/sc-jar-integrations.html)
        * [ Android integration ](../../../predictions/port-pred/scoring-code/android.html)
      * [ Portable Prediction Server ](../../../predictions/port-pred/pps/index.html) [ Portable Prediction Server ](../../../predictions/port-pred/pps/index.html)
        * [ Portable Prediction Server configuration ](../../../predictions/port-pred/pps/portable-pps.html)
        * [ Portable Prediction Server running modes ](../../../predictions/port-pred/pps/pps-run-modes.html)
        * [ Portable batch predictions ](../../../predictions/port-pred/pps/portable-batch-predictions.html)
        * [ Custom model Portable Prediction Server ](../../../predictions/port-pred/pps/custom-pps.html)
      * [ DataRobot Prime (deprecated) ](../../../predictions/port-pred/prime/index.html)
    * [ Predictions testing ](../../../predictions/pred-test.html)
    * [ Predictions reference ](../../../predictions/pred-file-limits.html)
  * [ MLOps ](../../../mlops/index.html) [ MLOps ](../../../mlops/index.html)
    * [ Deployment ](../../../mlops/deployment/index.html) [ Deployment ](../../../mlops/deployment/index.html)
      * [ Deployment workflows ](../../../mlops/deployment/deploy-workflows/index.html) [ Deployment workflows ](../../../mlops/deployment/deploy-workflows/index.html)
        * [ DataRobot model in a DataRobot environment ](../../../mlops/deployment/deploy-workflows/dr-model-dr-env.html)
        * [ DataRobot model in a PPS ](../../../mlops/deployment/deploy-workflows/dr-model-pps-env.html)
        * [ Custom model in a DataRobot environment ](../../../mlops/deployment/deploy-workflows/cus-model-dr-env.html)
        * [ Custom model in a PPS ](../../../mlops/deployment/deploy-workflows/cus-model-pps-env.html)
        * [ Scoring Code in an external environment ](../../../mlops/deployment/deploy-workflows/ext-dr-model-ext-env.html)
        * [ Monitor an external model with the monitoring agent ](../../../mlops/deployment/deploy-workflows/ext-cus-model-ext-env.html)
      * [ Register models ](../../../mlops/deployment/registry/index.html) [ Register models ](../../../mlops/deployment/registry/index.html)
        * [ Model Registry ](../../../mlops/deployment/registry/reg-create.html)
        * [ Register DataRobot models ](../../../mlops/deployment/registry/dr-model-reg.html)
        * [ Register custom models ](../../../mlops/deployment/registry/reg-custom-models.html)
        * [ Register external models ](../../../mlops/deployment/registry/reg-external-models.html)
        * [ Deploy registered models ](../../../mlops/deployment/registry/reg-deploy.html)
        * [ View and manage registered models ](../../../mlops/deployment/registry/reg-action.html)
        * [ Generate model compliance documentation ](../../../mlops/deployment/registry/reg-compliance.html)
        * [ Extend compliance documentation with key values ](../../../mlops/deployment/registry/reg-key-values.html)
        * [ Custom jobs ](../../../mlops/deployment/registry/reg-custom-jobs.html)
        * [ Import model packages into MLOps ](../../../mlops/deployment/registry/reg-transfer.html)
        * [ Model logs for model packages (legacy) ](../../../mlops/deployment/registry/reg-model-pkg-logs.html)
      * [ Prepare custom models for deployment ](../../../mlops/deployment/custom-models/index.html) [ Prepare custom models for deployment ](../../../mlops/deployment/custom-models/index.html)
        * [ Custom Model Workshop ](../../../mlops/deployment/custom-models/custom-model-workshop/index.html) [ Custom Model Workshop ](../../../mlops/deployment/custom-models/custom-model-workshop/index.html)
          * [ Create custom inference models ](../../../mlops/deployment/custom-models/custom-model-workshop/custom-inf-model.html)
          * [ Manage custom model dependencies ](../../../mlops/deployment/custom-models/custom-model-workshop/custom-model-dependencies.html)
          * [ Manage custom model resources ](../../../mlops/deployment/custom-models/custom-model-workshop/custom-model-resource-mgmt.html)
          * [ Add custom model versions ](../../../mlops/deployment/custom-models/custom-model-workshop/custom-model-versions.html)
          * [ Add training data to a custom model ](../../../mlops/deployment/custom-models/custom-model-workshop/custom-model-training-data.html)
          * [ Add files from remote repos to custom models ](../../../mlops/deployment/custom-models/custom-model-workshop/custom-model-repos.html)
          * [ Test custom models ](../../../mlops/deployment/custom-models/custom-model-workshop/custom-model-test.html)
          * [ Manage custom models ](../../../mlops/deployment/custom-models/custom-model-workshop/custom-model-actions.html)
          * [ Register custom models ](../../../mlops/deployment/custom-models/custom-model-workshop/custom-model-reg.html)
          * [ Create custom model proxies for external models ](../../../mlops/deployment/custom-models/custom-model-workshop/ext-model-proxy.html)
          * [ GitHub Actions for custom models ](../../../mlops/deployment/custom-models/custom-model-workshop/custom-model-github-action.html)
        * [ Custom model environments ](../../../mlops/deployment/custom-models/custom-model-environments/index.html) [ Custom model environments ](../../../mlops/deployment/custom-models/custom-model-environments/index.html)
          * [ Drop-in environments ](../../../mlops/deployment/custom-models/custom-model-environments/drop-in-environments.html)
          * [ Custom environments ](../../../mlops/deployment/custom-models/custom-model-environments/custom-environments.html)
      * [ Prepare for external model deployment ](../../../mlops/deployment/ext-model-prep/index.html) [ Prepare for external model deployment ](../../../mlops/deployment/ext-model-prep/index.html)
        * [ Add external prediction environments ](../../../mlops/deployment/ext-model-prep/ext-pred-env.html)
        * [ Manage prediction environments ](../../../mlops/deployment/ext-model-prep/ext-pred-env-manage.html)
        * [ Register external models ](../../../mlops/deployment/ext-model-prep/ext-model-reg.html)
      * [ Manage prediction environments ](../../../mlops/deployment/prediction-env/index.html) [ Manage prediction environments ](../../../mlops/deployment/prediction-env/index.html)
        * [ Add DataRobot Serverless prediction environments ](../../../mlops/deployment/prediction-env/pred-env.html)
        * [ Add external prediction environments ](../../../mlops/deployment/prediction-env/ext-pred-env.html)
        * [ Manage prediction environments ](../../../mlops/deployment/prediction-env/pred-env-manage.html)
        * [ Deploy a model to a prediction environment ](../../../mlops/deployment/prediction-env/pred-env-deploy.html)
        * [ Prediction environment integrations ](../../../mlops/deployment/prediction-env/pred-env-integrations/index.html) [ Prediction environment integrations ](../../../mlops/deployment/prediction-env/pred-env-integrations/index.html)
          * [ Automated deployment and replacement of Scoring Code in AzureML ](../../../mlops/deployment/prediction-env/pred-env-integrations/azureml-sc-deploy-replace.html)
          * [ Automated deployment and replacement in Sagemaker ](../../../mlops/deployment/prediction-env/pred-env-integrations/sagemaker-cm-deploy-replace.html)
      * [ Deploy models ](../../../mlops/deployment/deploy-methods/index.html) [ Deploy models ](../../../mlops/deployment/deploy-methods/index.html)
        * [ Deploy DataRobot models ](../../../mlops/deployment/deploy-methods/deploy-model.html)
        * [ Deploy custom models ](../../../mlops/deployment/deploy-methods/deploy-custom-inf-model.html)
        * [ Deploy external models ](../../../mlops/deployment/deploy-methods/deploy-external-model.html)
        * [ Configure deployment settings ](../../../mlops/deployment/deploy-methods/add-deploy-info.html)
        * [ Add prediction data post-deployment ](../../../mlops/deployment/deploy-methods/add-prediction-data-post-deploy.html)
      * [ MLOps agents ](../../../mlops/deployment/mlops-agent/index.html) [ MLOps agents ](../../../mlops/deployment/mlops-agent/index.html)
        * [ Monitoring agent ](../../../mlops/deployment/mlops-agent/monitoring-agent/index.html) [ Monitoring agent ](../../../mlops/deployment/mlops-agent/monitoring-agent/index.html)
          * [ Installation and configuration ](../../../mlops/deployment/mlops-agent/monitoring-agent/agent.html)
          * [ Examples directory ](../../../mlops/deployment/mlops-agent/monitoring-agent/agent-ex.html)
          * [ Monitoring agent use cases ](../../../mlops/deployment/mlops-agent/monitoring-agent/agent-use.html)
          * [ Environment variables ](../../../mlops/deployment/mlops-agent/monitoring-agent/env-var.html)
          * [ Library and agent spooler configuration ](../../../mlops/deployment/mlops-agent/monitoring-agent/spooler.html)
          * [ Download Scoring Code ](../../../mlops/deployment/mlops-agent/monitoring-agent/agent-sc.html)
          * [ Monitoring external multiclass deployments ](../../../mlops/deployment/mlops-agent/monitoring-agent/agent-multi.html)
        * [ Management agent ](../../../mlops/deployment/mlops-agent/mgmt-agent/index.html) [ Management agent ](../../../mlops/deployment/mlops-agent/mgmt-agent/index.html)
          * [ Installation and configuration ](../../../mlops/deployment/mlops-agent/mgmt-agent/mgmt-agent-install.html)
          * [ Configure environment plugins ](../../../mlops/deployment/mlops-agent/mgmt-agent/mgmt-agent-plugins.html)
          * [ Install the management agent for Kubernetes ](../../../mlops/deployment/mlops-agent/mgmt-agent/mgmt-agent-kubernetes.html)
          * [ Management agent deployment status and events ](../../../mlops/deployment/mlops-agent/mgmt-agent/mgmt-agent-events-status.html)
          * [ Relaunch deployments ](../../../mlops/deployment/mlops-agent/mgmt-agent/mgmt-agent-relaunch.html)
          * [ Force delete deployments ](../../../mlops/deployment/mlops-agent/mgmt-agent/mgmt-agent-delete.html)
        * [ Agent event log ](../../../mlops/deployment/mlops-agent/agent-event-log.html)
    * [ Deployment settings ](../../../mlops/deployment-settings/index.html) [ Deployment settings ](../../../mlops/deployment-settings/index.html)
      * [ Set up service health monitoring ](../../../mlops/deployment-settings/service-health-settings.html)
      * [ Set up data drift monitoring ](../../../mlops/deployment-settings/data-drift-settings.html)
      * [ Set up accuracy monitoring ](../../../mlops/deployment-settings/accuracy-settings.html)
      * [ Set up fairness monitoring ](../../../mlops/deployment-settings/fairness-settings.html)
      * [ Set up humility rules ](../../../mlops/deployment-settings/humility-settings.html)
      * [ Configure retraining ](../../../mlops/deployment-settings/retraining-settings.html)
      * [ Configure challengers ](../../../mlops/deployment-settings/challengers-settings.html)
      * [ Configure predictions settings ](../../../mlops/deployment-settings/predictions-settings.html)
      * [ Enable data exploration ](../../../mlops/deployment-settings/data-exploration-settings.html)
      * [ Set up custom metrics monitoring ](../../../mlops/deployment-settings/custom-metrics-settings.html)
      * [ Set up timeliness tracking ](../../../mlops/deployment-settings/usage-settings.html)
    * [ Lifecycle management ](../../../mlops/manage-mlops/index.html) [ Lifecycle management ](../../../mlops/manage-mlops/index.html)
      * [ Deployment inventory ](../../../mlops/manage-mlops/deploy-inventory.html)
      * [ Manage deployments ](../../../mlops/manage-mlops/actions-menu.html)
      * [ Replace deployed models ](../../../mlops/manage-mlops/deploy-replace.html)
      * [ Manage Automated Retraining policies ](../../../mlops/manage-mlops/set-up-auto-retraining.html)
    * [ Performance monitoring ](../../../mlops/monitor/index.html) [ Performance monitoring ](../../../mlops/monitor/index.html)
      * [ Overview tab ](../../../mlops/monitor/dep-overview.html)
      * [ Accuracy tab ](../../../mlops/monitor/deploy-accuracy.html)
      * [ Data Drift tab ](../../../mlops/monitor/data-drift.html)
      * [ Service Health tab ](../../../mlops/monitor/service-health.html)
      * [ Challengers tab ](../../../mlops/monitor/challengers.html)
      * [ Usage tab ](../../../mlops/monitor/deploy-usage.html)
      * [ Data Exploration tab ](../../../mlops/monitor/data-exploration.html)
      * [ Custom Metrics tab ](../../../mlops/monitor/custom-metrics.html)
      * [ Segmented analysis ](../../../mlops/monitor/deploy-segment.html)
      * [ Batch monitoring ](../../../mlops/monitor/deploy-batch-monitor.html)
      * [ Generative model monitoring ](../../../mlops/monitor/generative-model-monitoring.html)
    * [ Governance ](../../../mlops/governance/index.html) [ Governance ](../../../mlops/governance/index.html)
      * [ Model deployment approval workflow ](../../../mlops/governance/dep-admin.html)
      * [ Governance lens ](../../../mlops/governance/gov-lens.html)
      * [ Notifications tab ](../../../mlops/governance/deploy-notifications.html)
      * [ Humility tab ](../../../mlops/governance/humble.html)
      * [ Fairness tab ](../../../mlops/governance/mlops-fairness.html)
      * [ Deployment reports ](../../../mlops/governance/deploy-reports.html)
    * [ MLOps preview features ](../../../mlops/mlops-preview/index.html) [ MLOps preview features ](../../../mlops/mlops-preview/index.html)
      * [ Service Health and Accuracy history ](../../../mlops/mlops-preview/pp-deploy-history.html)
      * [ Automated deployment and replacement of Scoring Code in Snowflake ](../../../mlops/mlops-preview/pp-snowflake-sc-deploy-replace.html)
      * [ Run the monitoring agent in DataRobot ](../../../mlops/mlops-preview/monitoring-agent-in-dr.html)
      * [ Feature cache for Feature Discovery deployments ](../../../mlops/mlops-preview/safer-ft-cache.html)
      * [ MLOps reporting for unstructured models ](../../../mlops/mlops-preview/mlops-unstructured-models.html)
    * [ MLOps FAQ ](../../../mlops/mlops-faq.html)
  * [ Notebooks ](../../../dr-notebooks/index.html) [ Notebooks ](../../../dr-notebooks/index.html)
    * [ Manage notebooks ](../../../dr-notebooks/manage-nb/index.html) [ Manage notebooks ](../../../dr-notebooks/manage-nb/index.html)
      * [ Add notebooks ](../../../dr-notebooks/manage-nb/dr-create-nb.html)
      * [ Notebook settings ](../../../dr-notebooks/manage-nb/dr-settings-nb.html)
      * [ Notebook versioning ](../../../dr-notebooks/manage-nb/dr-revise-nb.html)
    * [ Notebook coding experience ](../../../dr-notebooks/code-nb/index.html) [ Notebook coding experience ](../../../dr-notebooks/code-nb/index.html)
      * [ Environment management ](../../../dr-notebooks/code-nb/dr-env-nb.html)
      * [ Create and execute cells ](../../../dr-notebooks/code-nb/dr-cell-nb.html)
      * [ Cell actions ](../../../dr-notebooks/code-nb/dr-action-nb.html)
      * [ Code intelligence ](../../../dr-notebooks/code-nb/dr-code-int.html)
      * [ Notebook terminals ](../../../dr-notebooks/code-nb/dr-terminal-nb.html)
      * [ Azure OpenAI Service integration ](../../../dr-notebooks/code-nb/dr-openai-nb.html)
    * [ Notebook reference ](../../../dr-notebooks/dr-notebook-ref.html)
  * [ AI Apps ](../../../app-builder/index.html) [ AI Apps ](../../../app-builder/index.html)
    * [ Create applications ](../../../app-builder/create-app.html)
    * [ Manage applications ](../../../app-builder/current-app.html)
    * [ Edit no-code applications ](../../../app-builder/edit-apps/index.html) [ Edit no-code applications ](../../../app-builder/edit-apps/index.html)
      * [ Pages ](../../../app-builder/edit-apps/app-pages.html)
      * [ Widgets ](../../../app-builder/edit-apps/app-widgets.html)
      * [ What-if and Optimizer ](../../../app-builder/edit-apps/whatif-opt.html)
      * [ Settings ](../../../app-builder/edit-apps/app-settings.html)
    * [ Use no-code applications ](../../../app-builder/use-apps/index.html) [ Use no-code applications ](../../../app-builder/use-apps/index.html)
      * [ Make predictions ](../../../app-builder/use-apps/app-make-pred.html)
      * [ View prediction results ](../../../app-builder/use-apps/app-analyze-result.html)
    * [ Time series applications ](../../../app-builder/ts-app.html)
    * [ Custom apps ](../../../app-builder/custom-apps/index.html) [ Custom apps ](../../../app-builder/custom-apps/index.html)
      * [ Upload custom applications ](../../../app-builder/custom-apps/app-upload-custom.html)
      * [ Host custom applications ](../../../app-builder/custom-apps/custom-apps-hosting.html)
      * [ Manage custom applications ](../../../app-builder/custom-apps/manage-custom-apps.html)
    * [ AI App reference ](../../../app-builder/reference/index.html) [ AI App reference ](../../../app-builder/reference/index.html)
      * [ Default widgets ](../../../app-builder/reference/default-widgets.html)
      * [ Optional widgets ](../../../app-builder/reference/optional-widgets.html)
    * [ AI App preview features ](../../../app-builder/app-preview/index.html) [ AI App preview features ](../../../app-builder/app-preview/index.html)
      * [ Prefill application templates ](../../../app-builder/app-preview/app-prefill.html)
      * [ Feature Discovery support in No-Code AI Apps ](../../../app-builder/app-preview/app-ft-cache.html)
  * [ Integrations ](../../../integrations/index.html) [ Integrations ](../../../integrations/index.html)
    * [ AWS ](../../../integrations/aws/index.html) [ AWS ](../../../integrations/aws/index.html)
      * [ Import data from AWS S3 ](../../../integrations/aws/import-from-aws-s3.html)
      * [ Deploy models on AWS EKS ](../../../integrations/aws/deploy-dr-models-on-aws.html)
      * [ Path-based routing to PPS ](../../../integrations/aws/path-based-routing-to-pps-on-aws.html)
      * [ Score Snowflake data on AWS EMR Spark ](../../../integrations/aws/score-snowflake-aws-emr-spark.html)
      * [ Ingest data with AWS Athena ](../../../integrations/aws/ingest-athena.html)
      * [ AWS Lambda ](../../../integrations/aws/lambda/index.html) [ AWS Lambda ](../../../integrations/aws/lambda/index.html)
        * [ AWS Lambda reporting to MLOps ](../../../integrations/aws/lambda/aws-lambda-reporting-to-mlops.html)
        * [ Use DataRobot Prime models with AWS Lambda ](../../../integrations/aws/lambda/prime-lambda.html)
        * [ Use Scoring Code with AWS Lambda ](../../../integrations/aws/lambda/sc-lambda.html)
      * [ Amazon SageMaker ](../../../integrations/aws/sagemaker/index.html) [ Amazon SageMaker ](../../../integrations/aws/sagemaker/index.html)
        * [ Deploy models on SageMaker ](../../../integrations/aws/sagemaker/sagemaker-deploy.html)
        * [ Use Scoring Code with AWS SageMaker ](../../../integrations/aws/sagemaker/sc-sagemaker.html)
    * [ Azure ](../../../integrations/azure/index.html) [ Azure ](../../../integrations/azure/index.html)
      * [ Run Batch Prediction jobs from Azure Blob Storage ](../../../integrations/azure/azure-blob-storage-batch-pred.html)
      * [ Deploy and monitor DataRobot models in Azure Kubernetes Service ](../../../integrations/azure/aks-deploy-and-monitor.html)
      * [ Deploy and monitor Spark models with DataRobot MLOps ](../../../integrations/azure/spark-deploy-and-monitor.html)
      * [ Deploy and monitor ML.NET models with DataRobot MLOps ](../../../integrations/azure/mlnet-deploy-and-monitor.html)
      * [ Use Scoring Code with Azure ML ](../../../integrations/azure/sc-azureml.html)
    * [ Google ](../../../integrations/google/index.html) [ Google ](../../../integrations/google/index.html)
      * [ Deploy and monitor models on GCP ](../../../integrations/google/google-cloud-platform.html)
      * [ Deploy the MLOps agent on GKE ](../../../integrations/google/mlops-agent-with-gke.html)
    * [ Snowflake ](../../../integrations/snowflake/index.html) [ Snowflake ](../../../integrations/snowflake/index.html)
      * [ Data ingest and project creation ](../../../integrations/snowflake/sf-project-creation.html)
      * [ Real-time predictions ](../../../integrations/snowflake/sf-client-scoring.html)
      * [ Server-side model scoring ](../../../integrations/snowflake/sf-server-scoring.html)
      * [ Snowflake external functions and streams ](../../../integrations/snowflake/sf-function-streams.html)
      * [ Generate Snowflake UDF Scoring Code ](../../../integrations/snowflake/snowflake-sc.html)

[For Self-Managed AI Platform users running v11.1, see the on-premise platform
documentation __](/11.1/en/docs/index.html)

Create custom tasks

  * Understand custom tasks 
    * Components of a custom task 
    * Task types 
    * Use a custom task 
    * Understand task content 
      * custom.py/custom.R 
      * model-metadata.yaml 
      * requirements.txt 
  * Define task code 
    * init() 
      * init() example 
      * init() input 
      * init() output 
    * fit() 
      * fit() examples 
      * How fit() works 
      * How to use fit() 
      * fit() input parameters 
      * fit() output 
    * load_model() 
      * load_model() example 
      * load_model() input 
      * load_model() output 
    * predict() 
      * predict() examples 
      * predict() input 
      * predict() output 
    * predict_proba() 
      * predict_proba() examples 
      * predict_proba() input 
      * predict_proba() output 
    * transform() 
      * transform() example 
      * transform() input 
      * transform() output 
  * Define task metadata 
  * Define the task environment 
  * Test the task locally 
    * Prerequisites 
    * Test compatibility with DataRobot 
    * Test task logic 
  * Upload the task 
    * Updating code 
  * Compose and train a blueprint 
    * Single-task blueprint 
    * Multitask blueprint 
  * Get insights 
    * Built-in insights 
    * Custom insights 
  * Deploy 
  * Download training artifacts 
  * Implicit sharing 

[Modeling](../../index.html "Modeling") > [Specialized
workflows](../index.html "Specialized workflows") > [Composable ML](index.html
"Composable ML") > Create custom tasks

# Create custom tasks¶

While DataRobot provides hundreds of built-in tasks, there are situations
where you need preprocessing or modeling methods that are not currently
supported out-of-the-box. To fill this gap, you can bring a custom task that
implements a missing method, plug that task into a blueprint inside DataRobot,
and then train, evaluate, and deploy that blueprint in the same way as you
would for any DataRobot-generated blueprint. (You can review how the process
works [here](cml-overview.html#how-it-works).)

The following sections describe creating and applying custom tasks and working
with the resulting custom blueprints.

## Understand custom tasks¶

The following helps to understand, generally, what a task is and how to use
it. It then provides an overview of task content.

### Components of a custom task¶

To bring and use a task, you need to define two components—the task’s content
and a container environment where the task’s content will run:

  * The task content (described on this page) is code written in Python or R. To be correctly parsed by DataRobot, the code must follow certain criteria. (Optional) You can add files that will be uploaded and used together with the task’s code (for example, you might want to add a separate file with a dictionary if your custom task contains text preprocessing).

  * The [container environment](cml-custom-env.html) is defined using a Docker file, and additional files, that will allow DataRobot to build an image where the task will run. There are a variety of built-in environments; users only need to build their own environment when they need to install Linux packages.

[![](../../../images/cml-task-9.png)](../../../images/cml-task-9.png)

At a high level, the steps to define a custom task include:

  1. Define and test task content locally (i.e., on your computer).
  2. (Optional) Create a container environment where the task will run.
  3. Upload the task content and environment (if applicable) into DataRobot.

### Task types¶

When creating a task, you must choose the one most appropriate for your
project. DataRobot leverages two types of tasks—estimators and
transforms—similar to sklearn. See the blueprint modification page to learn
[how these tasks work](cml-blueprint-edit.html#blueprint-task-types).

### Use a custom task¶

Once a task is uploaded, you can:

  * [Create and train a blueprint](cml-quickstart.html#apply-new-task-and-train) that contains that custom task. The blueprint will then appear on the project’s Leaderboard and can be used just like any other blueprint—in just a few clicks, you can compare it with other models, access model-agnostic insights, and deploy, monitor, and govern the resulting model.

  * [Share a task](../../../mlops/deployment/custom-models/custom-model-workshop/custom-model-actions.html) explicitly within your organization in the same way you share an environment. This can be particularly useful when you want to re-use the task in a future project. Additionally, because recipients don’t need to read and understand the task's code in order to use it, it can be applied by less technical colleagues. Custom tasks are also implicitly shared when a project or blueprint is shared.

### Understand task content¶

To define a custom task, create a local folder containing the files listed in
the table below (detailed descriptions follow the table).

Tip

You can find examples of these files in the [DataRobot task template
repository](https://github.com/datarobot/datarobot-user-
models/tree/master/task_templates) on GitHub.

[![](../../../images/cml-task-10.png)](../../../images/cml-task-10.png)

File | Description | Required  
---|---|---  
`custom.py` or `custom.R` | The task code that DataRobot will run in training and predictions. | Yes  
`model-metadata.yaml` | A file describing task's metadata, including input/output data requirements. | Required for custom transform tasks when a custom task outputs non-numeric data. If not provided, a default schema is used.  
`requirements.txt` | A list of Python or R packages to add to the base environment. | No  
Additional files | Other files used by the task (for example, a file that defines helper functions used inside `custom.py`). | No  
  
#### `custom.py/custom.R`¶

The `custom.py`/`custom.R` file defines a custom task. It must contain the
methods (functions) that enable DataRobot to correctly run the code and
integrate it with other capabilities.

#### `model-metadata.yaml`¶

For a custom task, you can supply a schema that can then be used to validate
the task when building and training a blueprint. A schema lets you specify
whether a custom task supports or outputs:

  * Certain data types
  * Missing values
  * Sparse data
  * A certain number of columns

#### `requirements.txt`¶

Use the `requirements.txt` file to pre-install Python or R packages that the
custom task is using but are not a part of the base environment.

Python exampleR example

For Python, provide a list of packages with their versions (1 package per
row). For example:

    
    
    numpy>=1.16.0, <1.19.0
    pandas==1.1.0
    scikit-learn==0.23.1
    lightgbm==3.0.0
    gensim==3.8.3
    sagemaker-scikit-learn-extension==1.1.0
    

For R, provide a list of packages without versions (1 package per row). For
example:

    
    
    dplyr
    stats
    

## Define task code¶

To define a custom task using DataRobot’s framework, your code must meet
certain criteria:

  * It must have a `custom.py` or `custom.R` file.

  * The `custom.py`/`custom.R` file must have methods, such as `fit()`, `score()`, or `transform()`, that define how a task is trained and how it scores new data. These are provided as interface classes or hooks. DataRobot automatically calls each one and passes the parameters based on the project and blueprint configuration. However, you have full flexibility to define the logic that runs inside each method.

View [an example on GitHub](https://github.com/datarobot/datarobot-user-
models/blob/master/task_templates/1_transforms/1_python_missing_values/custom.py)
of a task implementing missing values imputation using a median.

Note

Log in to GitHub before accessing these GitHub resources.

The following table lists the available methods. Note that most tasks only
require the `fit()` method. Classification tasks (binary or multiclass) must
have `predict_proba()`, regression tasks require `predict()`, and transforms
must have `transform()`. Other functions can be omitted.

Method | Purpose  
---|---  
`init()` | Load R libraries and files (R only, can be omitted for Python).  
`fit()` | Train an estimator/transform task and store it in an artifact file.  
`load_model()` | Load the trained estimator/transform from the artifact file.  
`predict` or `predict_proba` (For hook, use `score()`) | Define the logic used by a custom estimator to generate predictions.  
`transform()` | Define the logic used by a custom transform to generate transformed data.  
  
The schema below illustrates how methods work together in a custom task. In
some cases, some methods can be omitted, although `fit()` is always required
during training.

[![](../../../images/cml-task-1.png)](../../../images/cml-task-1.png)

The following sections describe each function, with examples.

### `init()`¶

The `init` method allows the task to load libraries and additional files for
use in other methods. It is required when using R but can typically be skipped
with Python.

#### `init()` example¶

The following provides a brief code snippet using `init()`; see a more
complete example [here](https://github.com/datarobot/datarobot-user-
models/blob/master/task_templates/2_estimators/5_r_binary_classification/custom.R).

R example

    
    
    init <- function(code_dir) {
       library(tidyverse)
       library(caret)
       library(recipes)
       library(gbm)
       source(file.path(code_dir, 'create_pipeline.R'))
    }
    

#### `init()` input¶

Input parameter | Description  
---|---  
`code_dir` | A link to the folder where the code is stored.  
  
#### `init()` output¶

The `init()` method does not return anything.

### `fit()`¶

`fit()` must be implemented for any custom task.

#### `fit()` examples¶

The following provides a brief code snippet using `fit()`; see a more complete
example [here](https://github.com/datarobot/datarobot-user-
models/blob/master/task_templates/2_estimators/4_python_binary_classification/custom.py).

Python exampleR example

The following is a Python example of `fit()` implementing Logistic Regression:

    
    
    def fit(X, y, output_dir, class_order, row_weights):
     estimator = LogisticRegression()
     estimator.fit(X, y)
    
     output_dir_path = Path(output_dir)
     if output_dir_path.exists() and output_dir_path.is_dir():
         with open("{}/artifact.pkl".format(output_dir), "wb") as fp:
             pickle.dump(estimator, fp)
    

The following is an example of R creating a regression model:

    
    
    fit <- function(X, y, output_dir, class_order=NULL, row_weights=NULL){
       model <- create_pipeline(X, y, 'regression')
    
      model_path <- file.path(output_dir, 'artifact.rds')
      saveRDS(model, file = model_path)
    }
    

#### How `fit()` works¶

DataRobot runs `fit()` when a custom estimator/transform is being trained. It
creates an artifact file (e.g., a `.pkl` file) where the trained object, such
as a trained sklearn model, is stored. The trained object is loaded from the
artifact and then passed as a parameter to `score()` and `transform()` when
scoring data.

#### How to use `fit()`¶

To use, train and put a trained object into an artifact file (e.g., `.pkl`)
inside the `fit()` function. The trained object must contain the information
or logic used to score new data. Some examples of trained objects:

  * A fitted [sklearn estimator](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html).
  * A median of training data, for a missing value imputation using a median. When scoring new data, it is used to replace missing values.

DataRobot automatically uses training/validation/holdout partitions based on
project settings.

#### `fit()` input parameters¶

The `fit()` task takes the following parameters:

Input parameters | Description  
---|---  
`X` | A pandas DataFrame (Python) or R data.frame (R) containing data the task receives during training.  
`y` | A pandas Series (Python) or R vector/factor (R) containing project's target data.  
`output_dir` | A path to the output folder. The artifact containing the trained object must be saved to this folder. You can also save other files there and once the blueprint is trained, all files added into that folder during fit are downloadable via the UI using the Artifact Download.  
`class_order` | _Only passed for a binary classification estimator_. A list containing the names of classes. The first entry is the class that is considered negative inside DataRobot's project; the second class is the class that is considered positive.  
`row_weights` | _Only passed in estimator tasks_. A list of weights passed when the project uses weights or smart downsampling.  
`**kwargs` | Not currently used but maintained for future compatibility.  
  
#### `fit()` output¶

Notes on `fit()` output:

  * `fit()` does not return anything, but it creates an artifact containing the trained object.

  * When no trained object is required (for example, a transform task implementing log transformation), create an “artificial” artifact by storing a number or a string in an artifact file. Otherwise (if `fit()` doesn't output an artifact), you must use `load_model`, which makes the task more complex.

  * The artifact must be saved into the `output_dir` folder.

  * The artifact can use any format.

  * Some formats are natively supported. When `output_dir` contains exactly one artifact file in a natively supported format, DataRobot automatically picks that artifact when scoring/transforming data. This way, you do not need to write a custom `load_model` method.

  * Natively supported formats include:

    * Python: `.pkl`, `.pth`, `.h5`, `.joblib`
    * Java: `.mojo`
    * R: `.rds`

### `load_model()`¶

The `load_model()` method loads one or more trained objects from the
artifact(s). It is only required when a trained object is stored in an
artifact that uses an unsupported format or when multiple artifacts are used.
`load_model()` is not required when there is a single artifact in one of the
supported formats:

  * Python: `.pkl`, `.pth`, `.h5`, `.joblib`
  * Java: `.mojo`
  * R: `.rds`

#### `load_model()` example¶

The following provides a brief code snippet using `load_model()`; see a more
complete example [here](https://github.com/datarobot/datarobot-user-
models/blob/master/task_templates/3_pipelines/14_python3_keras_joblib/custom.py).

Python exampleR example

In the following example, replace `deserialize_artifact` with an actual
function you use to parse the artifact:

    
    
    def load_model(code_dir: str):
        return deserialize_artifact(code_dir)
    
    
    
    load_model <- function(code_dir) {
       return(deserialize_artifact(code_dir))
    }
    

#### `load_model()` input¶

Input parameter | Description  
---|---  
`code_dir` | A link to the folder where the artifact is stored.  
  
#### `load_model()` output¶

The `load_model()` method returns a trained object (of any type).

### `predict()`¶

The `predict()` method defines how DataRobot uses the trained object from
`fit()` to score new data. DataRobot runs this method when the task is used
for scoring inside a blueprint. This method is only usable for regression and
anomaly tasks. Note that for R, instead use the `score()` hook outlined in the
examples below.

#### `predict()` examples¶

The following provides a brief code snippet using `predict()`; see a more
complete example [here](https://github.com/datarobot/datarobot-user-
models/blob/master/task_templates/2_estimators/1_python_regression/custom.py#L45).

Python examplesR examples

Python example for a regression or anomaly estimator:

    
    
    def predict(self, data: pd.DataFrame, **kwargs):
        return pd.DataFrame(data=self.estimator.predict(data), columns=["Predictions"])
    

R example for a regression or anomaly estimator:

    
    
    score <- function(data, model, ...) {
      return(data.frame(Predictions = predict(model, newdata=data, type = response")))
    }
    

R example for a binary estimator:

    
    
    score <- function(data, model, ...) {
      scores <- predict(model, data, type = "response")
      scores_df <- data.frame('c1' = scores, 'c2' = 1- scores)
      names(scores_df) <- c("class1", "class2")
      return(scores_df)
    }
    

#### `predict()` input¶

Input parameter | Description  
---|---  
`data` | A pandas DataFrame (Python) or R data.frame (R) containing the data the custom task will score.  
`**kwargs` | Not currently used but maintained for future compatibility. (For R, use `score(data, model, …)`)  
  
#### `predict()` output¶

Notes on `predict()` output:

  * Returns a pandas DataFrame (or R data.frame/[tibble](https://tibble.tidyverse.org/)).

  * For regression or anomaly detection projects, the output must contain a single numeric column named **Predictions**.

### `predict_proba()`¶

The `predict_proba()` method defines how DataRobot uses the trained object
from `fit()` to score new data. This method is only usable for binary and
multiclass tasks. DataRobot runs this method when the task is used for scoring
inside a blueprint. Note that for R, you instead use the `score()` hook used
in the examples below.

#### `predict_proba()` examples¶

The following provides a brief code snippet using `predict_proba()`; see a
more complete example [here](https://github.com/datarobot/datarobot-user-
models/blob/master/task_templates/2_estimators/4_python_binary_classification/custom.py#L40).

Python examplesR examples

Python example for a binary or multiclass estimator:

    
    
    def predict_proba(self, data: pd.DataFrame, **kwargs) -> pd.DataFrame:
        return pd.DataFrame(
            data=self.estimator.predict_proba(data), columns=self.estimator.classes_
        )
    

R example for a regression or anomaly estimator:

    
    
    score <- function(data, model, ...) {
      return(data.frame(Predictions = predict(model, newdata=data, type = response")))
    }
    

R example for a binary estimator:

    
    
    score <- function(data, model, ...) {
      scores <- predict(model, data, type = "response")
      scores_df <- data.frame('c1' = scores, 'c2' = 1- scores)
      names(scores_df) <- c("class1", "class2")
      return(scores_df)
    }
    

#### `predict_proba()` input¶

Input parameter | Description  
---|---  
`data` | A pandas DataFrame (Python) or R data.frame (R) containing the data the custom task will score.  
`**kwargs` | Not currently used but maintained for future compatibility. (For R, use `score(data, model, …)`)  
  
#### `predict_proba()` output¶

Notes on `predict_proba()` output:

  * Returns a pandas DataFrame (or R data.frame/tibble).

  * For binary or multiclass projects, output must have one column per class, with class names used as column names. Each cell must contain the probability of the respective class, and each row must sum up to 1.0.

### `transform()`¶

The `transform()` method defines the output of a custom transform and returns
transformed data. Do not use this method for estimator tasks.

#### `transform()` example¶

The following provides a brief code snippet using `transform()`; see a more
complete example [here](https://github.com/datarobot/datarobot-user-
models/blob/master/task_templates/1_transforms/1_python_missing_values/custom.py).

Python exampleR example

A Python example that creates a transform and outputs to a dataframe:

    
    
    def transform(X: pd.DataFrame, transformer) -> pd.DataFrame:
      return transformer.transform(X)
    
    
    
    transform <- function(X, transformer, ...){
       X_median <- transformer
    
       for (i in 1:ncol(X)) {
       X[is.na(X[,i]), i] <- X_median[i]
       }
         X
    }
    

#### `transform()` input¶

Input parameter | Description  
---|---  
`X` | A pandas DataFrame (Python) or R data.frame (R) containing data the custom task should transform.  
`transformer` | A trained object loaded from the artifact (typically, a trained transformer).  
`**kwargs` | Not currently used but maintained for future compatibility.  
  
#### `transform()` output¶

The `transform()` method returns a pandas DataFrame or R data.frame with
transformed data.

## Define task metadata¶

To define metadata, create a `model-metadata.yaml` file and put it in the top
level of the task/model directory. The file specifies additional information
about a custom task and is described in detail [here](../../../reference/pred-
ai-ref/cml-ref/cml-validation.html).

## Define the task environment¶

There are multiple options for defining the environment where a custom task
runs. You can:

  * Choose from a variety of built-in environments.

  * If a built-in environment is missing Python or R packages, add missing packages by specifying them in the task's `requirements.txt` file. If provided, `requirements.txt` must be uploaded together with `custom.py` or `custom.R`in the task content. If task content contains subfolders, it must be placed in the top folder.

  * You can [build your own environment](cml-custom-env.html) if you need to install Linux packages.

## Test the task locally¶

While it is not a requirement that you test the task locally before uploading
it to DataRobot, it is strongly recommended. Validating functionality in
advance can save much time and debugging in the future.

A custom task must meet the following basic requirements to be successful:

  * The task is compatible with DataRobot requirements and can be used to build a blueprint.
  * The task works as intended (for example, a transform produces the output you need).

Use `drum fit` in the command line to quickly run and test your task. It will
automatically validate that the task meets DataRobot requirements. To test
that the task works as intended, combine `drum fit` with other popular
debugging methods, such as printing output to a terminal or file.

[![](../../../images/cml-task-2.png)](../../../images/cml-task-2.png)

### Prerequisites¶

To test your task:

  * Put the task's content into a single folder.

  * [Install DRUM](cml-drum.html). Ensure that the Python environment where DRUM is installed is activated. Preferably, also install [Docker Desktop](https://www.docker.com/products/docker-desktop).

  * Create a CSV file with test data you can use when testing a task.

  * Because you will use the command line to run tests, open a terminal window.

### Test compatibility with DataRobot¶

The following provides an example of using `drum fit` to test whether a task
is compatible with DataRobot blueprints. To learn more about using `drum fit`,
type `drum fit --help` in the command line.

For a custom task (estimator or transform), use the following basic command in
your terminal. Replace placeholder names in `< >` brackets with actual paths
and names. Note that the following options are available for TARGET_TYPE:

  * For estimators: binary, multiclass, regression, anomaly
  * For transforms: transform

    
    
    drum fit --code-dir <folder_with_task_content> --input <test_data.csv>  --target-type <TARGET_TYPE> --target <target_column_name> --docker <folder_with_dockerfile> --verbose
    

Note that the `target` parameter should be omitted when it is not used during
training (for example, in case of anomaly detection estimators or some
transform tasks). In that case, a command could look like this:

    
    
    drum fit --code-dir <folder_with_task_content> --input <test_data.csv>  --target-type anomaly --docker <folder_with_dockerfile> --verbose
    

### Test task logic¶

To confirm a task works as intended, combine `drum fit` with other debugging
methods, such as adding "print" statements into the task's code:

  * Add `print(msg)` into one of the methods; when running a task using `drum fit`, DataRobot will print the message in the terminal.
  * Write intermediate or final results into a local file for later inspections, which could help to confirm that a custom task works as expected.

[![](../../../images/cml-task-3.png)](../../../images/cml-task-3.png)

## Upload the task¶

Once a task's content is defined, upload it into DataRobot to use it to build
and train a blueprint. Uploading a custom task into DataRobot involves three
steps:

  1. Create a new task in the [Model Registry](cml-quickstart.html#create-a-custom-task).
  2. Select a container environment where the task will run.
  3. [Upload the task content](cml-quickstart.html#upload-task-content).

Once uploaded, the custom task appears in the list of tasks available to the
blueprint editor.

[![](../../../images/cml-task-4.png)](../../../images/cml-task-4.png)

### Updating code¶

You can always upload updated code. To avoid conflicts, DataRobot creates a
new version each time code is uploaded. When creating a blueprint, you can
select the specific task version to use in your blueprint.

[![](../../../images/cml-task-5.png)](../../../images/cml-task-5.png)

## Compose and train a blueprint¶

Once a custom task is created, there are two options for composing a blueprint
that uses the task:

  * Compose a single-task blueprint, using only the task (estimator only) that you created.
  * Create a multitask blueprint using the [blueprint editor](cml-blueprint-edit.html).

### Single-task blueprint¶

If your custom estimator task contains all the necessary training code, you
can build and train a single-task blueprint. To do so, navigate to **Model
Registry > Custom Model Workshop > Tasks**. Select the task and click **Train
new model** :

[![](../../../images/cml-task-8.png)](../../../images/cml-task-8.png)

When complete, a blueprint containing the selected task appears in the
project's Leaderboard.

### Multitask blueprint¶

To compose a blueprint containing more than one task, use the blueprint
editor. Below is a summary of the steps; see [the documentation](cml-
blueprint-edit.html) for complete details.

  1. From the project Leaderboard, **Repository** , or **Blueprints** tab in the AI Catalog, select a blueprint to use as a template for your new blueprint.

  2. Navigate to the **Blueprint** view and start editing the selected blueprint.

  3. Select an existing task or add a new one, then select a custom task from the dropdown of built-in and custom tasks.

  4. Save and then train the new blueprint by clicking **Train**. A model containing the selected task appears in the project's Leaderboard.

## Get insights¶

You can use DataRobot insights to help evaluate the models that result from
your custom blueprints.

### Built-in insights¶

Once a blueprint is trained, it appears in the project Leaderboard where you
can easily compare accuracy with other models. [Metrics and model-agnostic
insights](cml-quickstart.html#evaluate-and-deploy) are available just as for
DataRobot models.

### Custom insights¶

You can generate custom insights by creating artifacts during training.
Additionally, you can generate insights using the Predictions API, just as for
any other Leaderboard model.

Tip

Custom insights are additional views that help to understand how a model
works. They may come in the form of a visualization or a CSV file. For
example, if you wanted to leverage [LIME's model-agnostic
insights](https://cran.r-project.org/web/packages/lime/index.html), you could
import that package, run it on the trained model in the `custom.py` or other
helper files, and then write out the resulting model insights.

## Deploy¶

Once a model containing a custom task is trained, it can be [deployed,
monitored, and governed](../../../api/dev-learning/python/mlops/index.html)
just like any other DataRobot model.

[![](../../../images/cml-task-6.png)](../../../images/cml-task-6.png)

## Download training artifacts¶

When training a blueprint with a custom task, DataRobot creates an artifact
available for download. Any file that is put into `output_dir` inside `fit()`
of a custom task becomes a part of the artifact. You can use the artifact to:

  * Generate custom insights during training. For this, generate file(s) (such as image or text files) as a part of the `fit()` function. Write them to `output_dir`.

  * Download a trained model (for example, as a `.pkl`file) that you can then load locally to generate additional insights or to deploy outside of DataRobot.

To download an artifact for a model, navigate to **Predict > Downloads >
Artifact Download**.

[![](../../../images/cml-task-7.png)](../../../images/cml-task-7.png)

You can also [download the code of any environment](cml-custom-env.html#share-
and-download) you have access to. To download, click on an environment, select
the version, and click **Download**.

## Implicit sharing¶

A task or environment is not available in the model registry to other users
unless it was explicitly shared. That does not, however, limit users' ability
to use blueprints that include that task. This is known as _implicit sharing_.

For example, consider a project shared by User A and User B. If User A creates
a new task, and then creates a blueprint using that task, User B can still
interact with that blueprint (clone, modify, rerun, etc.) regardless of
whether they have _Read_ access to any custom task within that blueprint.
Because every task is associated with an environment, implicit sharing applies
to environments as well. User A can also [explicitly
share](../../../mlops/deployment/custom-models/custom-model-workshop/custom-
model-actions.html) just the task or environment, as needed.

Implicit sharing is unique permission model that grants _Execute_ access to
everyone in the custom task author’s organization. When a user has access to a
blueprint (but not necessarily explicit access to a custom task in that
blueprint) _Execute_ access allows:

  * Interacting with the resulting model. For example, retraining, running Feature Impact and Feature Effects, deploying, and making batch predictions.

  * Cloning and editing a blueprint from the shared project, and then saving the blueprint as their own.

  * Viewing and downloading Leaderboard logs.

Some capabilities that Execute access does not allow include:

  * Downloading the custom task artifact.

  * Viewing, modifying, or deleting the custom task from the model registry.

  * Using the task in another blueprint. (Instead you would clone the blueprint containing the task and edit the blueprint and/or task.)

Back to top

